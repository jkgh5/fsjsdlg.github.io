{
  "id": 2676,
  "project_id": 642,
  "upload_types": [
    "repo",
    "zip"
  ],
  "file_filter_regex": "\\A(?!(((.*/)?(__MACOSX|\\.git|node_modules|bower_components|jspm_packages|\\.idea|build|.ipynb_checkpoints|\\.Trash-0|logs)(\\Z|/))))((.*\\.(js|css|py|html|htm|txt|md|markdown|sql|swift|java|gradle|xml|rst|yml|yaml|rmd|pdf|docx|h|H|hh|hxx|h\\+\\+|c|C|cc|cpp|cxx|c\\+\\+)\\Z)|((.*/)?(README|Readme|readme|Makefile)\\Z))",
  "nomination_eligible": false,
  "stand_out": "- Implement FFT (Fast Fourier transform) algorithm to generate the frequency of SF service calls.\n- Try to scale out and figure out the combinations on your local machine number of offsets/partitions. Can you make 2000 offsets per trigger? How can you tell you’re ingesting <offsets> per trigger? What tools can you use?\n- Generate dynamic charts/images per batch.",
  "hide_criteria": false,
  "created_at": "2019-08-12T16:33:21.282Z",
  "updated_at": "2020-04-07T05:29:02.528Z",
  "hashtag": "",
  "max_upload_size_mb": 500,
  "estimated_sla": null,
  "project_assistant_enabled": false,
  "available_for_cert_project": false,
  "language": "en-us",
  "ndkeys": [
    "nd029-beta",
    "nd029",
    "nd029-cn",
    "nd029-ent"
  ],
  "coursekeys": [],
  "is_career": false,
  "sections": [
    {
      "id": 5726,
      "name": "Implement Apache Kafka Server",
      "created_at": "2019-08-12T17:13:31.652Z",
      "updated_at": "2019-08-12T17:14:22.169Z",
      "deleted_at": null,
      "position": 0,
      "rubric_id": 2676,
      "rubric_items": [
        {
          "id": 16420,
          "section_id": 5726,
          "passed_description": "These two files - `kafka/config/server.properties` and `kafka/config/zookeeper.properties` - should have the correct port for the topics, as evidenced by matching ports in the 2 files `server.properties` and `producer_server.py`. (E.g., if the student used 9092 for the topic ports in the `producer_server.py`, these files should have changed). Additionally, the host for kafka/config/server.properties should be changed to localhost. ",
          "exceeded_description": null,
          "created_at": "2019-08-12T17:14:22.341Z",
          "updated_at": "2019-10-24T03:31:14.204Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "Configure Zookeeper/Kafka server correctly. \nWrite Kafka Producer code correctly.",
          "exceedable": false
        },
        {
          "id": 16421,
          "section_id": 5726,
          "passed_description": "The consumer-console logs from the terminal imply that the student has set up the Kafka/Zookeeper server and Kafka topic correctly (Kafka host/port number matches in the code and the configuration files, topic name is written out in the `producer_server.py`, and the same topic is created for Kafka producer). The logs show the byte stream from the Kafka server is being consumed by the consumer if the proper consumer is given. \n\nStudent has implemented a Kafka consumer in `consumer_server.py` and demonstrated that the consumer is able to consume data. The screenshot of the kafka-consumer-console output demonstrates that the students were able to send out a byte stream from Kafka Producer, and the rows should be in json (or python dict) format.",
          "exceeded_description": null,
          "created_at": "2019-08-12T17:15:17.219Z",
          "updated_at": "2019-11-22T05:32:14.695Z",
          "deleted_at": null,
          "optional": false,
          "position": 1,
          "criteria": "Create a properly working Apache Kafka server that is producing data. Create a visualization of the data produced by the Kafka server by creating a consumer server.",
          "exceedable": false
        }
      ]
    },
    {
      "id": 5727,
      "name": "Implement Apache Spark Structured Streaming",
      "created_at": "2019-08-12T17:16:55.721Z",
      "updated_at": "2019-08-12T17:17:08.327Z",
      "deleted_at": null,
      "position": 1,
      "rubric_id": 2676,
      "rubric_items": [
        {
          "id": 16422,
          "section_id": 5727,
          "passed_description": "Use various configs in SparkSession, set up for local mode (using local[*]) and Spark readstream configurations. The student should be using at least format (‘kafka’), ‘kafka.bootstrap.servers’, and ‘subscribe’. Other configuration properties such as ‘startingOffsets’ and ‘maxRatePerPartition’ are optional.  This should act as a broker internally tied with Spark, and the student should be able to consume the data that they generated from the local Kafka server to the Spark Structured Streaming. \n\nStudent demonstrates in their answer to the first question that they understand how changing values of the SparkSession property parameters affects the throughput and latency of their data.\n\nThe screenshot of the Spark UI on the Streaming tab demonstrates that the student used proper properties to render the Spark UI and monitor their jobs.",
          "exceeded_description": null,
          "created_at": "2019-08-12T17:17:08.546Z",
          "updated_at": "2019-11-22T05:32:14.703Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "Set up SparkSession in local mode and Spark configurations.",
          "exceedable": false
        },
        {
          "id": 16423,
          "section_id": 5727,
          "passed_description": "From the data that is coming in from the Kafka producer, the student should be investigating the schema of the data (key, value, topic, partition, offset, timestamp, timestampType). The student should successfully select the relevant column (value) from the data and investigate the JSON object (key-value pair).\n\nUsing this information about the JSON object, the student should create a proper schema for Apache Spark Structured Streaming. This schema will check data types, and will be used with Spark Catalyst Optimizers.\n",
          "exceeded_description": null,
          "created_at": "2019-08-12T17:18:34.286Z",
          "updated_at": "2019-11-22T05:32:14.708Z",
          "deleted_at": null,
          "optional": false,
          "position": 1,
          "criteria": "Select relevant input from the Kafka producer.\nBuild a schema for JSON input.\n",
          "exceedable": false
        },
        {
          "id": 16424,
          "section_id": 5727,
          "passed_description": "Aggregator/Join logic should show different types of crimes occurred in certain time frames (using window functions, group by original_crime_time), and how many calls occurred in certain time frames (group by on certain time frame, but the student will need to create a user defined function internally for Spark to change time format first to do this operation). Any aggregator or join logic should be performed with Watermark API. The students can play around with the time frame - this is part of their analytical exercises and data exploration.\n\nStudent demonstrates in their response to question 2 that they understand, given a resource and system, how to find out the most optimal configuration for their application.\n\nThe screenshot of the progress reporter demonstrates that the student correctly ingested the data in a micro-batch fashion and was able to monitor each micro-batch through the console.",
          "exceeded_description": null,
          "created_at": "2019-08-12T17:20:08.398Z",
          "updated_at": "2019-11-20T18:16:01.691Z",
          "deleted_at": null,
          "optional": false,
          "position": 2,
          "criteria": "Write various statistical analytics using Spark Structured Streaming APIs, and analyze and monitor the status of the current micro-batch through progress reporter.",
          "exceedable": false
        }
      ]
    }
  ],
  "project": {
    "id": 642,
    "name": "SF Crime Statistics with Spark Streaming",
    "nanodegree_key": "nd029",
    "is_cert_project": false,
    "audit_project_id": null,
    "hashtag": null,
    "audit_rubric_id": 2899,
    "entitlement_required": false,
    "is_career": false,
    "recruitment_family_id": 5,
    "created_at": "2019-08-19T21:13:22.911Z",
    "updated_at": "2021-03-29T08:52:36.548Z",
    "price": "6.0",
    "ungradeable_price": "3.0",
    "audit_price": "0.0"
  }
}