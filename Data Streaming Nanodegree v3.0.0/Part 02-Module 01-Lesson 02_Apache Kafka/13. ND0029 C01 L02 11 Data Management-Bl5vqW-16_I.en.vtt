WEBVTT
Kind: captions
Language: en

00:00:01.080 --> 00:00:05.050
Kafka topics have a few options for how they store data.

00:00:05.049 --> 00:00:07.570
First, let's review data retention.

00:00:07.570 --> 00:00:11.320
Data retention is how Kafka actually stores the data in a topic.

00:00:11.320 --> 00:00:15.370
Some topics expire data after a certain period of time.

00:00:15.369 --> 00:00:16.809
When the data expires,

00:00:16.809 --> 00:00:20.169
Kafka will eventually delete the data from the log on disk.

00:00:20.170 --> 00:00:24.460
For example, if the retention time period for a topic is seven days,

00:00:24.460 --> 00:00:28.080
after seven days, any data in the topic,

00:00:28.079 --> 00:00:30.294
older than that time period will be removed.

00:00:30.295 --> 00:00:34.164
Some topics expire data when a certain topic size is reached.

00:00:34.164 --> 00:00:36.670
For example, the topic has a retention policy of

00:00:36.670 --> 00:00:40.185
no more than one gigabyte and when the one gigabyte threshold is met,

00:00:40.185 --> 00:00:43.660
the oldest data in that topic will be deleted.

00:00:43.659 --> 00:00:46.074
So in this case, we've had two gigabytes,

00:00:46.075 --> 00:00:47.620
we may remove that log.

00:00:47.619 --> 00:00:51.789
Some topics use a strategy of both time and size.

00:00:51.789 --> 00:00:54.265
In this case, whichever threshold has hit first

00:00:54.265 --> 00:00:57.160
is used to trigger log clean up and deletion.

00:00:57.159 --> 00:01:01.549
A third option exists known as a compacted topic.

00:01:01.549 --> 00:01:05.935
In this scenario, there is no size or time limit for data in the topic.

00:01:05.935 --> 00:01:10.375
Instead, the message key is used to identify messages uniquely.

00:01:10.375 --> 00:01:12.340
The duplicate key is found,

00:01:12.340 --> 00:01:16.040
the latest value for that key is kept in the old messages deleted.

00:01:16.040 --> 00:01:19.660
Additionally, if a duplicate key is found with a null body,

00:01:19.659 --> 00:01:22.914
the key and the value will be deleted from the topic entirely.

00:01:22.915 --> 00:01:29.975
So here you'll notice we have the key a multiple times throughout our logs.

00:01:29.974 --> 00:01:32.015
We don't see any other repeated keys.

00:01:32.015 --> 00:01:33.709
So when compaction happens,

00:01:33.709 --> 00:01:35.644
only one of these logs will be kept.

00:01:35.644 --> 00:01:40.119
As you can see, these messages are going to be deleted and removed.

00:01:40.120 --> 00:01:42.310
Now, we have only one copy of a.

00:01:42.310 --> 00:01:43.859
When creating a topic,

00:01:43.859 --> 00:01:47.465
you may also choose a set of compression algorithm on the topic.

00:01:47.465 --> 00:01:50.060
Compression can help reduce network overhead in

00:01:50.060 --> 00:01:54.385
your cluster as well as reduce the amount of data stored on disk in your broker's.

00:01:54.385 --> 00:01:57.390
Kafka supports lz4, ztsd,

00:01:57.390 --> 00:02:00.015
snappy, gzip and no compression.

00:02:00.015 --> 00:02:03.829
Trade offs in these compression algorithms are essentially out of scope for now.

00:02:03.829 --> 00:02:05.649
We're going to review them briefly in a few sections,

00:02:05.650 --> 00:02:08.664
but for now, just know that compression exists in these formats.

00:02:08.664 --> 00:02:12.129
Finally, a common question that comes up with data management topics is

00:02:12.129 --> 00:02:15.294
whether to allow more than one event type in a topic.

00:02:15.294 --> 00:02:18.759
For example, maybe you have a purchases topic that includes purchases from

00:02:18.759 --> 00:02:20.859
a clothing store and another that includes

00:02:20.860 --> 00:02:23.260
purchases from another company that sells groceries.

00:02:23.259 --> 00:02:25.209
More than likely, these two companies will have

00:02:25.210 --> 00:02:27.700
different ideas of what a purchase event looks like.

00:02:27.699 --> 00:02:30.219
This means that all downstream clients will also

00:02:30.219 --> 00:02:32.844
have to know the difference between this purchase event types as well.

00:02:32.844 --> 00:02:35.469
This is a bad practice and an anti-pattern.

00:02:35.469 --> 00:02:37.840
It's a recipe for production outages.

00:02:37.840 --> 00:02:42.310
I strongly recommend that you do not mix different types of events in a given stream.

00:02:42.310 --> 00:02:46.194
There is very little overhead to adding more topics to a Kafka broker.

00:02:46.194 --> 00:02:48.639
But there is a lot of programmatic overhead in parsing

00:02:48.639 --> 00:02:52.129
through different types of events in a single Kafka topic.

