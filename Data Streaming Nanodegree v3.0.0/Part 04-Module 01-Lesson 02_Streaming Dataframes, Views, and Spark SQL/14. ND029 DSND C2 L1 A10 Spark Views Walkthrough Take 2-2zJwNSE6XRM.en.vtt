WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.060
Let's take a look at creating and querying a spark view,

00:00:03.060 --> 00:00:05.340
and sinking that information to Kafka.

00:00:05.340 --> 00:00:08.820
The first thing I want to is see if Kafka is up and running.

00:00:08.820 --> 00:00:12.465
Now if you've been using the workspace included with the course,

00:00:12.465 --> 00:00:16.830
a lot of times you don't have to start Kafka again because it's already running.

00:00:16.830 --> 00:00:21.300
Especially if you've already done an exercise without leaving your computer,

00:00:21.300 --> 00:00:23.085
it's probably still up.

00:00:23.085 --> 00:00:27.400
The way to check this is running ps-ef.

00:00:27.710 --> 00:00:33.120
Here, I see several Java processes running that tells me Kafka is up.

00:00:33.120 --> 00:00:37.950
The next thing I want to do is go ahead and check if Spark is running.

00:00:37.950 --> 00:00:40.465
It's the same command and I've already run it.

00:00:40.465 --> 00:00:43.115
Here I'm going to look through the output and see

00:00:43.115 --> 00:00:46.280
two different processes, both running Spark.

00:00:46.280 --> 00:00:51.820
That tells me that Spark is up and running. Now we're ready to go.

00:00:51.820 --> 00:00:54.800
I've imported the SparkSession library,

00:00:54.800 --> 00:00:59.840
so I can go ahead and start coding by creating a SparkSession.

00:00:59.840 --> 00:01:01.880
First I'm going to say,

00:01:01.880 --> 00:01:05.584
create that session and I'm going to name it spark.

00:01:05.584 --> 00:01:09.580
I'm going to call my app name gear-position.

00:01:09.580 --> 00:01:13.355
Now this name will only show up in the Spark logs.

00:01:13.355 --> 00:01:18.690
It's not too important but we might as well name it something useful.

00:01:18.690 --> 00:01:21.755
Now we're going to set the LogLevel to warn,

00:01:21.755 --> 00:01:24.890
which reduces a lot of unnecessary output.

00:01:24.890 --> 00:01:30.290
Last, I'm going to take the SparkSession I've created and connect to Kafka.

00:01:30.290 --> 00:01:36.385
When doing that, I will get a data frame that represents the Kafka connection.

00:01:36.385 --> 00:01:40.320
We're going to call that data frame something useful.

00:01:40.320 --> 00:01:46.810
We'll call it gearPositionRawStreamingDF for data frame.

00:01:46.850 --> 00:01:51.005
We're going to read it from a spark connection.

00:01:51.005 --> 00:01:54.200
We're going to access the readStream.

00:01:54.200 --> 00:01:57.740
We want it to talk to Kafka.

00:01:57.740 --> 00:02:00.745
We're going to see dot format Kafka.

00:02:00.745 --> 00:02:04.615
Now, we need to tell it where the Kafka server is running.

00:02:04.615 --> 00:02:09.155
That's where ours is running on our computer, on our server.

00:02:09.155 --> 00:02:11.915
Here, we need this topic to match

00:02:11.915 --> 00:02:15.890
exactly the topic that we know the data will be coming from.

00:02:15.890 --> 00:02:18.170
The name of our topic is gear-position,

00:02:18.170 --> 00:02:21.800
and this needs to match it without any change at all.

00:02:21.800 --> 00:02:26.120
We want to read all the information that Kafka has in this topic.

00:02:26.120 --> 00:02:29.285
We're going to say, "Give me the earliest."

00:02:29.285 --> 00:02:32.160
I'm just missing that character.

00:02:32.160 --> 00:02:35.755
Then we're going to tell to load that dataframe.

00:02:35.755 --> 00:02:40.310
Now this dataframe represents the data exactly as it came from Kafka,

00:02:40.310 --> 00:02:41.885
which is in binary format.

00:02:41.885 --> 00:02:46.240
We want to create another dataframe that's not in binary format.

00:02:46.240 --> 00:02:51.510
We're going to go to the gearPositionRawStreamingDF,

00:02:51.510 --> 00:02:56.965
and we're going to select the data in a format that's not binary.

00:02:56.965 --> 00:03:01.555
The key in this case is a string when we're done with it,

00:03:01.555 --> 00:03:03.620
and it's the trackId.

00:03:03.620 --> 00:03:08.035
Then the value is going to be a string as well.

00:03:08.035 --> 00:03:13.329
The key and value are fields that come by default when we read from Kafka,

00:03:13.329 --> 00:03:15.640
in case you're wondering what those are from.

00:03:15.640 --> 00:03:19.000
We're renaming both of them to something meaningful.

00:03:19.000 --> 00:03:22.585
In this case, the value is our gearPosition.

00:03:22.585 --> 00:03:28.970
Now we've got a dataframe that has meaningful fields that are in string format.

00:03:28.970 --> 00:03:35.715
I've got that gearPositionStreamingdataframe and I'm going to save it as a view.

00:03:35.715 --> 00:03:39.000
I've created that view called GearPosition.

00:03:39.000 --> 00:03:41.390
Now I can select the data from it.

00:03:41.390 --> 00:03:44.105
Now I can use any valid SQL statement,

00:03:44.105 --> 00:03:49.925
and my favorite SQL statement is by far the select star statement.

00:03:49.925 --> 00:03:53.395
It's often used for ad hoc querying.

00:03:53.395 --> 00:03:55.695
This is kind of an ad hoc query.

00:03:55.695 --> 00:03:59.465
We're just wanting to see what data we got from Kafka to start out with.

00:03:59.465 --> 00:04:03.995
I'm going to say, select star from GearPosition.

00:04:03.995 --> 00:04:08.465
Now I've got that dataframe that has the results of that query.

00:04:08.465 --> 00:04:09.740
It should have two fields.

00:04:09.740 --> 00:04:14.115
What are they? TrackId and gearPosition.

00:04:14.115 --> 00:04:17.220
Those two fields are in this dataframe.

00:04:17.220 --> 00:04:21.260
I'm going to go ahead and cast

00:04:21.260 --> 00:04:25.745
that data back into the field names that Kafka wants them as.

00:04:25.745 --> 00:04:29.240
What are the two field names that Kafka gave me by default?

00:04:29.240 --> 00:04:34.940
You guys remember trackId was the one I named it,

00:04:34.940 --> 00:04:38.320
but it actually was called key.

00:04:38.320 --> 00:04:42.335
We're going to give it to Kafka in a way that it can understand.

00:04:42.335 --> 00:04:44.420
It understands keys and values.

00:04:44.420 --> 00:04:47.430
We're going to call trackId, key.

00:04:47.430 --> 00:04:52.939
We're going to cast both the trackId and the gearPosition as strings.

00:04:52.939 --> 00:04:55.415
It will actually convert them to binary

00:04:55.415 --> 00:04:58.960
eventually but we're passing them as strings in the beginning.

00:04:58.960 --> 00:05:01.550
The other field we need to pass is value.

00:05:01.550 --> 00:05:03.605
So we're going to pass it a value.

00:05:03.605 --> 00:05:06.005
That's our gearPosition.

00:05:06.005 --> 00:05:10.819
Now we're going to go in reverse back into Kafka,

00:05:10.819 --> 00:05:12.200
so instead of a readStream,

00:05:12.200 --> 00:05:14.030
we have a writeStream.

00:05:14.030 --> 00:05:18.670
We're still going to have a format called Kafka.

00:05:18.670 --> 00:05:23.530
We still need to provide the Bootstrap options.

00:05:23.530 --> 00:05:26.220
We're going to give it a localhost.

00:05:26.220 --> 00:05:29.420
We're sending it back to the same broker that we got from

00:05:29.420 --> 00:05:32.180
in the beginning but we are putting it in

00:05:32.180 --> 00:05:35.180
a different topic because this data is going to

00:05:35.180 --> 00:05:38.690
be in theory enriched data from the original.

00:05:38.690 --> 00:05:43.945
The topic we're creating as a new topic called gear-position-updates.

00:05:43.945 --> 00:05:50.725
We're going to use another option that we haven't seen before called checkpointLocation.

00:05:50.725 --> 00:05:54.740
CheckpointLocation needs to be a file path

00:05:54.740 --> 00:05:58.850
that the Spark worker has access to as a write,

00:05:58.850 --> 00:06:00.640
so it can actually write to it.

00:06:00.640 --> 00:06:05.525
This is just a temporary file that's used by Spark during processing.

00:06:05.525 --> 00:06:09.110
Because TMP is often a good place for

00:06:09.110 --> 00:06:13.310
temporary data and it usually has write access for every user,

00:06:13.310 --> 00:06:15.295
I'm going to use /tmp.

00:06:15.295 --> 00:06:20.330
Now we're going to tell it to start streaming and we're going to awaitTermination,

00:06:20.330 --> 00:06:22.860
which means, run indefinitely.

00:06:23.180 --> 00:06:27.305
I'm going to go into the home workspace,

00:06:27.305 --> 00:06:30.440
spark directory and list.

00:06:30.440 --> 00:06:35.450
I have a script already for this called submit-gear-position.sh.

00:06:35.450 --> 00:06:38.410
I'm going to run that script.

00:06:38.410 --> 00:06:42.960
The main point of it is to make the command easier to run.

00:06:43.970 --> 00:06:46.320
We're getting this started,

00:06:46.320 --> 00:06:49.160
and it's pulling in all the Kafka libraries that it needs.

00:06:49.160 --> 00:06:53.795
You can see that and some other libraries.

00:06:53.795 --> 00:06:57.550
We have an error on line 6.

00:06:57.550 --> 00:07:05.085
It says, "The builder object has no attribute sparkContext."

00:07:05.085 --> 00:07:08.730
The problem is right here.

00:07:08.730 --> 00:07:13.400
I've got just a builder and I haven't actually told it to create it.

00:07:13.400 --> 00:07:16.535
The builder alone isn't a session.

00:07:16.535 --> 00:07:19.625
We have to tell the builder to create the session.

00:07:19.625 --> 00:07:24.120
So I've added that and we'll repeat our command.

00:07:24.140 --> 00:07:27.405
Now we're waiting for output.

00:07:27.405 --> 00:07:32.010
It says that, the cast value string gearPosition,

00:07:32.010 --> 00:07:34.905
says there's an error in that,

00:07:34.905 --> 00:07:39.630
input string and it was expecting a paren,

00:07:39.630 --> 00:07:46.620
cast value string gearPosition.

00:07:46.620 --> 00:07:49.770
It's talking about this one.

00:07:49.770 --> 00:07:56.175
It should say, cast a value as string gearPosition.

00:07:56.175 --> 00:07:59.860
Let's fix that and run it again.

00:08:01.250 --> 00:08:06.365
An error occurred, cannot resolve cast trackId as string key,

00:08:06.365 --> 00:08:15.190
given input positions, input columns, gearposition.trackId, gearposition.gearPosition.

00:08:15.190 --> 00:08:17.730
I'm just looking at my select statement.

00:08:17.730 --> 00:08:23.080
I selected everything from gearPosition into this gearPositionSelectStarDF.

00:08:23.740 --> 00:08:33.920
Given input columns, gearposition.trackId, gearposition.gearPosition.

00:08:33.920 --> 00:08:36.430
I guess normally what I would do is start looking at

00:08:36.430 --> 00:08:39.920
the raw streaming data frame because

00:08:39.920 --> 00:08:44.485
that's the value that we're having issues getting into a different format.

00:08:44.485 --> 00:08:46.350
What I think I'm going to,

00:08:46.350 --> 00:08:50.880
I'm take this and stream it to the console.

00:08:50.880 --> 00:08:54.830
I have a feeling we're going to learn something by doing this.

00:08:54.830 --> 00:08:57.730
I'm going to save that file.

00:08:57.730 --> 00:09:01.210
Now, I'm going to run it.

00:09:02.720 --> 00:09:08.160
We should see data up until this point.

00:09:08.160 --> 00:09:10.860
What that means is that

00:09:10.860 --> 00:09:19.115
the stream up until this point is working correctly,

00:09:19.115 --> 00:09:21.425
and so the data coming in is correct.

00:09:21.425 --> 00:09:24.520
This select is working right.

00:09:24.520 --> 00:09:30.320
It's most likely the casting that's happening here that's causing us issues.

00:09:30.320 --> 00:09:33.995
What I'm going to do is just for troubleshooting,

00:09:33.995 --> 00:09:38.810
I'm going to change this to key and this to value.

00:09:38.810 --> 00:09:41.665
Then, in this code,

00:09:41.665 --> 00:09:44.745
I'm going to comment out this one.

00:09:44.745 --> 00:09:53.360
I'm going to change key as key string key and value as string value.

00:09:53.360 --> 00:09:58.750
I'm just going to save that and run that again.

00:09:59.000 --> 00:10:02.900
Now let's see if the error is different, which it is.

00:10:02.900 --> 00:10:06.710
We know we've found the right thing that's failing.

00:10:06.710 --> 00:10:10.295
The thing that's failing is this select.

00:10:10.295 --> 00:10:11.930
Oh, the problem is,

00:10:11.930 --> 00:10:14.245
we're not using selectExpr.

00:10:14.245 --> 00:10:18.845
So it's in select expression instead of just dot select.

00:10:18.845 --> 00:10:21.175
I'm going to save that.

00:10:21.175 --> 00:10:24.525
Then we're going to run it again.

00:10:24.525 --> 00:10:29.135
Now you'll notice that the data is not printing out on the screen. Why is that?

00:10:29.135 --> 00:10:32.455
The data is not printing on the screen because we didn't tell it to.

00:10:32.455 --> 00:10:34.975
This is how we send it to the screen.

00:10:34.975 --> 00:10:37.475
We're sending it to a Kafka topic.

00:10:37.475 --> 00:10:40.425
Let's go look at the Kafka topic.

00:10:40.425 --> 00:10:42.630
I'm just clearing my screen.

00:10:42.630 --> 00:10:44.510
In your Kafka installation,

00:10:44.510 --> 00:10:46.115
you will have a folder.

00:10:46.115 --> 00:10:50.550
Now, this is how it is in the workspace but your Kafka will be in a different place.

00:10:50.550 --> 00:10:53.135
There's a sub-folder though called bin.

00:10:53.135 --> 00:10:57.500
If you list that, you'll find a thing called kafka-console-consumer.

00:10:57.500 --> 00:11:04.290
That's the utility that you can use to connect to a topic.

00:11:04.290 --> 00:11:11.490
I'm going to call it --bootstrap-server localhost 9092 that tells me that broker.

00:11:11.490 --> 00:11:14.730
I'm going to tell it the topic name.

00:11:14.730 --> 00:11:18.050
Then I'm going say from the beginning,

00:11:18.050 --> 00:11:20.645
and there's my values that I'm getting.

00:11:20.645 --> 00:11:24.190
You'll notice we don't see the keys like we did earlier.

00:11:24.190 --> 00:11:27.630
We saw the track numbers before trackids.

00:11:27.630 --> 00:11:30.260
But now we're just getting what's in this column.

00:11:30.260 --> 00:11:33.520
This utility only gives you the values from Kafka.

00:11:33.520 --> 00:11:36.695
That's what it looks like to take data from Kafka,

00:11:36.695 --> 00:11:38.285
massage it a little bit,

00:11:38.285 --> 00:11:40.370
and then stream it back to a Kafka topic.

00:11:40.370 --> 00:11:42.680
I'm going to kill this by saying Control C,

00:11:42.680 --> 00:11:43.985
and I've stopped that.

00:11:43.985 --> 00:11:47.395
I'm going to kill this one by saying Control C as well.

00:11:47.395 --> 00:11:50.780
Just remember when submitting a Kafka Spark application,

00:11:50.780 --> 00:11:53.254
you need the appropriate Kafka library.

00:11:53.254 --> 00:11:57.950
The version number of the library should match the version of Spark you're running,

00:11:57.950 --> 00:12:01.560
and the scripts that we've run have included that for you.

00:12:01.560 --> 00:12:04.670
In this case, we're running this library with Version

00:12:04.670 --> 00:12:09.055
2.4.6 because that matches the version of Spark that we're running.

00:12:09.055 --> 00:12:15.060
Now let's look at the solution to the ATM visits issue.

