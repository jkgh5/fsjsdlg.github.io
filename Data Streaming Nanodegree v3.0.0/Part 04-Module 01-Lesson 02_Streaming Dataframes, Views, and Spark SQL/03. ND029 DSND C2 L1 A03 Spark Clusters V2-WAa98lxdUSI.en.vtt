WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.970
Let's talk about using Spark clusters.

00:00:02.970 --> 00:00:06.000
In this lesson, we explore running a Spark cluster.

00:00:06.000 --> 00:00:11.295
We discuss languages that work with Spark and why Python is a great way to get started.

00:00:11.295 --> 00:00:16.560
We dive into a data pipeline in action and discuss the main components that make it tick.

00:00:16.560 --> 00:00:21.990
Lastly, we deploy or submit an application to a running Spark cluster.

00:00:21.990 --> 00:00:24.180
In the first part of this section,

00:00:24.180 --> 00:00:27.315
we talk about why you would use a server cluster.

00:00:27.315 --> 00:00:31.745
Spark can orchestrate from one to hundreds of worker nodes.

00:00:31.745 --> 00:00:34.520
ZooKeeper connects to the Spark master and is

00:00:34.520 --> 00:00:37.970
optional for high availability in a standalone cluster.

00:00:37.970 --> 00:00:41.285
In Kubernetes, you submit the Spark application

00:00:41.285 --> 00:00:45.145
using the URI for the Kubernetes cluster API.

00:00:45.145 --> 00:00:48.185
You can run Spark in a container platform like

00:00:48.185 --> 00:00:52.730
Kubernetes or on a standalone cluster of servers you provision.

00:00:52.730 --> 00:00:54.710
In a standalone cluster,

00:00:54.710 --> 00:00:57.470
you submit the Spark application using

00:00:57.470 --> 00:01:02.420
the Spark master URI or Universal Resource Identifier.

00:01:02.420 --> 00:01:06.200
The order that you start the Spark cluster in is important.

00:01:06.200 --> 00:01:09.520
The first step is starting this Spark master.

00:01:09.520 --> 00:01:13.570
Next, check the logs for the Spark master URI.

00:01:13.570 --> 00:01:16.000
Now, you are ready to start the worker,

00:01:16.000 --> 00:01:18.575
pointing to the master Spark URI.

00:01:18.575 --> 00:01:20.720
To start the Spark cluster,

00:01:20.720 --> 00:01:24.065
first run the start master.sh script.

00:01:24.065 --> 00:01:26.345
Watch for the log file,

00:01:26.345 --> 00:01:31.525
cut or tail the log file watching for the Spark URI in bold.

00:01:31.525 --> 00:01:38.675
Run the start slave.sh script with the spark URI immediately after the script name.

00:01:38.675 --> 00:01:41.660
These commands may seem like a lot of information.

00:01:41.660 --> 00:01:44.335
We will practice these commands a lot more.

00:01:44.335 --> 00:01:47.390
You just learned how to start up a Spark cluster.

00:01:47.390 --> 00:01:52.865
Now we will talk about why Python is an ideal language to write and deploy on Spark.

00:01:52.865 --> 00:01:57.005
Spark is polyglot, which means it supports multiple languages.

00:01:57.005 --> 00:02:00.005
You can write Spark Streaming applications using R,

00:02:00.005 --> 00:02:03.430
which is very popular for data science applications.

00:02:03.430 --> 00:02:04.970
You can also use Java,

00:02:04.970 --> 00:02:09.190
which is very familiar to a large portion of the development community.

00:02:09.190 --> 00:02:11.660
Spark also supports Scala,

00:02:11.660 --> 00:02:15.355
one of the foundational languages for Spark and big data.

00:02:15.355 --> 00:02:17.795
Lastly, Spark supports Python,

00:02:17.795 --> 00:02:20.740
one of the most popular programming languages.

00:02:20.740 --> 00:02:24.170
Data Streaming constructs vary from language to language.

00:02:24.170 --> 00:02:29.465
R uses DataFrames, Java and Scala use types datasets.

00:02:29.465 --> 00:02:32.100
Python uses DataFrames.

00:02:32.100 --> 00:02:36.465
Spark supports R, Java, Scala, and Python.

00:02:36.465 --> 00:02:39.530
Why use Python to write a data pipeline?

00:02:39.530 --> 00:02:42.065
Python is not strictly typed,

00:02:42.065 --> 00:02:45.905
which can make writing a data streaming application very easy.

00:02:45.905 --> 00:02:49.520
It is extremely popular for data science applications,

00:02:49.520 --> 00:02:51.460
which makes it a great choice.

00:02:51.460 --> 00:02:56.090
You just learned why Python is an ideal language to deploy on a Spark cluster.

00:02:56.090 --> 00:02:58.715
Next, you will learn about data pipelines.

00:02:58.715 --> 00:03:03.395
It is very rarely for purely academic reasons that we process data,

00:03:03.395 --> 00:03:05.480
usually we have a pressing need for

00:03:05.480 --> 00:03:09.515
some information to assist in the decisioning process of our business.

00:03:09.515 --> 00:03:14.345
The data we evaluate originates in one place and arrives in a different place.

00:03:14.345 --> 00:03:18.800
A data pipeline or Spark application delivers value by

00:03:18.800 --> 00:03:24.695
retrieving and then sending augmented or updated data onto another system.

00:03:24.695 --> 00:03:29.510
A data pipeline is a series of steps of processing that augment or

00:03:29.510 --> 00:03:34.345
refine a raw data source in preparation for consumption by another system.

00:03:34.345 --> 00:03:38.810
Every Spark application starts by reading in a data source.

00:03:38.810 --> 00:03:44.965
A series of steps is executed that transforms the original flow of information.

00:03:44.965 --> 00:03:50.815
The end result is a DataFrame that can be synced externally with an outside system.

00:03:50.815 --> 00:03:55.970
The source is continually pulled for updates to apply the transformations.

00:03:55.970 --> 00:03:58.370
This continual pulling is what sets

00:03:58.370 --> 00:04:02.365
Spark Streaming apart from non streaming applications.

00:04:02.365 --> 00:04:05.385
Here's a sample of Spark Streaming application.

00:04:05.385 --> 00:04:08.885
Note the import of SparkSession is omitted for brevity,

00:04:08.885 --> 00:04:11.780
we start by instantiating a SparkSession.

00:04:11.780 --> 00:04:14.180
This connects us to the Spark cluster.

00:04:14.180 --> 00:04:16.610
Next, we open a streaming source,

00:04:16.610 --> 00:04:18.310
Kafka in this case.

00:04:18.310 --> 00:04:21.455
Third, we apply data transformations.

00:04:21.455 --> 00:04:25.760
Last, we sink data to something external to Spark.

00:04:25.760 --> 00:04:30.040
The console is a good sink to test her data quality visually.

00:04:30.040 --> 00:04:33.920
To take advantage of the resources in the Spark cluster,

00:04:33.920 --> 00:04:36.500
we submit or deploy the Spark code.

00:04:36.500 --> 00:04:38.765
You will need to start the master,

00:04:38.765 --> 00:04:41.570
and the workers, and lastly,

00:04:41.570 --> 00:04:43.685
submit the application code.

00:04:43.685 --> 00:04:46.065
You just learned about data pipelines.

00:04:46.065 --> 00:04:48.935
Now we will talk about how to run one in Spark.

00:04:48.935 --> 00:04:51.440
To deploy your Spark application,

00:04:51.440 --> 00:04:53.435
first start the master,

00:04:53.435 --> 00:04:55.205
then start the worker.

00:04:55.205 --> 00:04:58.925
You should wait up to two minutes to see the results.

00:04:58.925 --> 00:05:02.660
When your Spark Streaming application begins producing data,

00:05:02.660 --> 00:05:05.540
it will push the data to the sink you of created.

00:05:05.540 --> 00:05:08.929
By default, Spark executes and micro batches.

00:05:08.929 --> 00:05:10.715
When sinking to the console,

00:05:10.715 --> 00:05:12.320
you will see the batches.

00:05:12.320 --> 00:05:14.000
When sinking to anything else,

00:05:14.000 --> 00:05:16.250
you won't see the batches on the console.

00:05:16.250 --> 00:05:20.260
You should wait up to two minutes to see the results.

