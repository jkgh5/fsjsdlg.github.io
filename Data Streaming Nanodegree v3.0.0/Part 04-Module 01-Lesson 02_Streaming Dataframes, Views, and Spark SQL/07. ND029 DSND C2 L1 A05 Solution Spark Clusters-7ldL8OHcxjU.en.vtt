WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.155
Let's look at the solution to the whole Spark exercise.

00:00:04.155 --> 00:00:09.315
The requirements might have seemed straight forward, counting some strings,

00:00:09.315 --> 00:00:12.690
but using Spark data frames can be a little bit

00:00:12.690 --> 00:00:17.445
challenging at times to do some traditional operations.

00:00:17.445 --> 00:00:19.725
Let's look at how it works.

00:00:19.725 --> 00:00:23.895
The first thing we're going to do is create a variable

00:00:23.895 --> 00:00:28.935
that references the home/workspace/Test.txt file.

00:00:28.935 --> 00:00:33.570
That might look familiar from the earlier work sheet that I did.

00:00:33.570 --> 00:00:38.880
I'm going to create a variable that stores the full path to the file called

00:00:38.880 --> 00:00:43.985
logFile because a lot of times that's what we're parsing, is log files.

00:00:43.985 --> 00:00:46.540
I'm going to create a Spark session,

00:00:46.540 --> 00:00:48.930
and the way I'm going do that is using

00:00:48.930 --> 00:00:53.270
the SparkSession class that I imported above, right here.

00:00:53.270 --> 00:00:56.090
It will be important to remember that,

00:00:56.090 --> 00:00:59.090
when you're writing your own Spark application that

00:00:59.090 --> 00:01:03.070
you want to make sure and import that class.

00:01:03.070 --> 00:01:05.370
I'm going to take the SparkSession.

00:01:05.370 --> 00:01:07.250
I'm going to access the builder on it,

00:01:07.250 --> 00:01:10.965
and then I'm going to invoke the appName method.

00:01:10.965 --> 00:01:15.545
I'm going to call getOrCreate to create my session.

00:01:15.545 --> 00:01:18.575
Now, a lot of times Spark is verbose,

00:01:18.575 --> 00:01:21.490
so we're going to just take the log level down.

00:01:21.490 --> 00:01:24.280
Now, I'm going to read in the file.

00:01:24.280 --> 00:01:27.680
Let's create a variable called logData.

00:01:27.680 --> 00:01:30.050
That's actually a data frame.

00:01:30.050 --> 00:01:33.210
We're using data frames.

00:01:33.590 --> 00:01:38.355
There is the data frame, logData.

00:01:38.355 --> 00:01:43.535
Now, we're going to create some variables to track.

00:01:43.535 --> 00:01:46.885
We're going to instantiate those at zero,

00:01:46.885 --> 00:01:51.590
and then we're going to do something a little interesting.

00:01:51.590 --> 00:01:54.845
We're going to create some functions.

00:01:54.845 --> 00:01:56.930
The first function I'm going to make,

00:01:56.930 --> 00:01:58.855
it's just called countA.

00:01:58.855 --> 00:02:01.845
It accepts a row as an input.

00:02:01.845 --> 00:02:04.435
That's a row from a data frame.

00:02:04.435 --> 00:02:09.155
Now I'm going to access the global variable that I instantiate earlier.

00:02:09.155 --> 00:02:18.165
I'm going to increment the counter for every time I see the letter A in that row.

00:02:18.165 --> 00:02:23.585
As I increment it on each data frame row,

00:02:23.585 --> 00:02:26.855
so think about it, every row there is in the data frame,

00:02:26.855 --> 00:02:29.060
will end up calling this function.

00:02:29.060 --> 00:02:32.840
This print statement will get called how many times?

00:02:32.840 --> 00:02:34.520
Let's see if we look at the file,

00:02:34.520 --> 00:02:37.715
there we've five lines in that file,

00:02:37.715 --> 00:02:41.720
and that means it's going to be printing five times.

00:02:41.720 --> 00:02:44.980
I'm just going to do a quick print command,

00:02:44.980 --> 00:02:47.570
and then I'm going to define the count for

00:02:47.570 --> 00:02:53.245
the letter B. I'm going to call that def countB.

00:02:53.245 --> 00:02:55.950
It accepts a row as well.

00:02:55.950 --> 00:03:00.335
We're going to reference the global count for that.

00:03:00.335 --> 00:03:03.100
We're going to increment as well.

00:03:03.100 --> 00:03:08.165
Again, we're just incrementing it by the row.value.count,

00:03:08.165 --> 00:03:12.305
and we're counting B on that row of the data frame.

00:03:12.305 --> 00:03:14.995
That should get called five times as well.

00:03:14.995 --> 00:03:19.940
Now, what I'm going do is I'm going to call for each on the data frame.

00:03:19.940 --> 00:03:24.805
I'm going to save forEach and now I'm parsing the function.

00:03:24.805 --> 00:03:26.970
How many times will it print?

00:03:26.970 --> 00:03:30.020
It will print five times and I'll see the A count go up.

00:03:30.020 --> 00:03:32.770
Well, I want do the same thing for B. What should I do?

00:03:32.770 --> 00:03:35.030
I should do the same thing again,

00:03:35.030 --> 00:03:39.600
but call parsing the countB function.

00:03:39.700 --> 00:03:45.185
Last, I'm going to stop Spark so that the whole thing can finish.

00:03:45.185 --> 00:03:47.995
I'm going to save that file.

00:03:47.995 --> 00:03:50.830
I need to make sure Spark is running,

00:03:50.830 --> 00:03:53.870
so the command I like to use is ps -ef.

00:03:53.870 --> 00:03:56.030
I do not see Spark running anywhere here.

00:03:56.030 --> 00:03:57.800
I don't see the word Spark.

00:03:57.800 --> 00:03:59.330
In order to get it running,

00:03:59.330 --> 00:04:02.225
I've got to do something about that.

00:04:02.225 --> 00:04:06.385
I'm going to change to the Spark sbin folder.

00:04:06.385 --> 00:04:09.070
I'm going to run start-master.

00:04:09.070 --> 00:04:12.170
I've now got a handle to the log file.

00:04:12.170 --> 00:04:14.000
I'm going to copy that.

00:04:14.000 --> 00:04:16.490
If I do this too quickly,

00:04:16.490 --> 00:04:19.600
I'll actually catch the log before it's done writing.

00:04:19.600 --> 00:04:24.615
But if I pause a second like I just did, I'll see this.

00:04:24.615 --> 00:04:28.790
That's the URI for the spark master.

00:04:28.790 --> 00:04:36.125
Now, I'm going to run start-worker and I'm going to call this URI.

00:04:36.125 --> 00:04:39.305
Now, if I do ps -ef,

00:04:39.305 --> 00:04:45.425
I now see two lines that are running Spark.

00:04:45.425 --> 00:04:48.095
You might only see that Java part,

00:04:48.095 --> 00:04:50.975
but Spark runs in a Java Runtime.

00:04:50.975 --> 00:04:54.730
We're going to just type clear to move up the terminal for view-ability.

00:04:54.730 --> 00:04:58.185
Now, I'm back up to the top of the terminal.

00:04:58.185 --> 00:05:06.725
Oh, and it looks like I did forget the print statement for the total B count.

00:05:06.725 --> 00:05:09.380
Let's put that in.

00:05:09.380 --> 00:05:13.930
There we go. Now, I'm going to save the Python file

00:05:13.930 --> 00:05:19.475
and now I'm going to run the Spark application.

00:05:19.475 --> 00:05:26.360
I'm going to change to the home/workspace/spark/bin folder and I'm going to

00:05:26.360 --> 00:05:31.310
run./spark-submit and then I'm going to give

00:05:31.310 --> 00:05:37.270
it the full path to the hellospark Python script.

00:05:38.000 --> 00:05:45.620
This is what we see when we're waiting for the output usually and it's not happy.

00:05:45.620 --> 00:05:47.735
Let's see, line 40,

00:05:47.735 --> 00:05:54.145
logData.forEach object has no attribute forEach.

00:05:54.145 --> 00:05:57.600
Of course, I did it in the wrong case,

00:05:57.600 --> 00:06:01.185
so forEach should just be like this.

00:06:01.185 --> 00:06:03.994
I'm going to update that file.

00:06:03.994 --> 00:06:06.800
I'm going to save it and then again I'm going to do

00:06:06.800 --> 00:06:12.225
spark-submit full path to the Python file rather.

00:06:12.225 --> 00:06:14.955
Now, here's where we wait for the output.

00:06:14.955 --> 00:06:21.240
This can take up to two minutes when we're running larger computations. There we go.

00:06:21.240 --> 00:06:23.685
Let's see how we did,

00:06:23.685 --> 00:06:27.810
so the A shows a total count of six.

00:06:27.810 --> 00:06:30.050
If we open up the file,

00:06:30.050 --> 00:06:31.805
let's see what we've got.

00:06:31.805 --> 00:06:39.480
We have, "Hello everyone this is the test text file we would like to parse."

00:06:39.480 --> 00:06:41.610
There's an A that's one.

00:06:41.610 --> 00:06:47.700
Then there's another A that's two, that's three,

00:06:47.700 --> 00:06:51.405
that's four, that's five,

00:06:51.405 --> 00:06:55.805
and I'm sure there's a sixth one that I'm missing somewhere in here.

00:06:55.805 --> 00:07:00.075
"Hello everyone this is the test text file we would like to parse," 1,

00:07:00.075 --> 00:07:03.135
2, 3, 4, 5,

00:07:03.135 --> 00:07:05.500
6. There we go.

00:07:06.230 --> 00:07:09.965
That is a little tricky compared to what we're used to doing.

00:07:09.965 --> 00:07:12.500
With loops in Python,

00:07:12.500 --> 00:07:15.065
we normally can access data directly.

00:07:15.065 --> 00:07:22.860
With Spark, we actually have to tell the data frame what to do and let it do it.

00:07:22.860 --> 00:07:26.630
The way we did that in this case is by defining a function

00:07:26.630 --> 00:07:31.385
and then telling it to do that on the data that was contained inside of the data frame.

00:07:31.385 --> 00:07:34.070
It's a little different than what we're used to doing,

00:07:34.070 --> 00:07:39.785
and so data streaming can be a new way of thinking about solving data problems.

00:07:39.785 --> 00:07:43.620
But it's fun and you can do a lot of really cool stuff with it.

