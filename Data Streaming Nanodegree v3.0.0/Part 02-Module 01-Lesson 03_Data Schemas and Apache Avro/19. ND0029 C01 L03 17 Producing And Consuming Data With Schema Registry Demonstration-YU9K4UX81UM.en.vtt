WEBVTT
Kind: captions
Language: en

00:00:01.429 --> 00:00:04.620
In this exercise, we'll update the work we did in

00:00:04.620 --> 00:00:07.425
our last exercise to use schema registry.

00:00:07.424 --> 00:00:08.929
So returning to our workspace,

00:00:08.929 --> 00:00:10.964
we're going to see familiar code here.

00:00:10.964 --> 00:00:13.469
We have our purchase or line item,

00:00:13.470 --> 00:00:18.914
and we have an existing schema with our line items type defined inside of it.

00:00:18.914 --> 00:00:23.160
Now, we're going to need to update our produce and consume functions a little bit.

00:00:23.160 --> 00:00:27.054
So first, we're going to go ahead and create a schema registry type.

00:00:27.054 --> 00:00:30.089
Now, this CachedSchemaRegistryClient is actually

00:00:30.089 --> 00:00:33.725
imported from confluent kafka-python library.

00:00:33.725 --> 00:00:35.885
As you can see on line 11.

00:00:35.884 --> 00:00:37.479
We've also, while we're up here,

00:00:37.479 --> 00:00:41.714
I want to point out imported the AvroConsumer and the AvroProducer.

00:00:41.715 --> 00:00:45.800
So returning to schema registry definition.

00:00:45.799 --> 00:00:50.539
We're going to go ahead and pass in the location of the broker,

00:00:50.539 --> 00:00:55.960
and the schema register URL is defined on line 18,

00:00:55.960 --> 00:01:00.740
and you'll also have the schema registry URL to find your exercise for you.

00:01:00.740 --> 00:01:02.149
So we simply pass it into

00:01:02.149 --> 00:01:05.060
the CachedSchemaRegistryClient to instantiate

00:01:05.060 --> 00:01:07.745
it and then we'll have a schema registry instance.

00:01:07.745 --> 00:01:11.195
We're going to pass that into our producer.

00:01:11.194 --> 00:01:16.429
So we're going to say schema registry equals schema registry instance.

00:01:16.430 --> 00:01:22.565
Next, we need to update our producer to actually send the schema data to the topic.

00:01:22.564 --> 00:01:25.629
So historically, we're using a producer type,

00:01:25.629 --> 00:01:28.954
we want to make sure that we use the AvroProducer.

00:01:28.954 --> 00:01:31.655
Then one of the major differences between

00:01:31.655 --> 00:01:37.370
the standard producer and an AvroProducer is that we have to specify schemas.

00:01:37.370 --> 00:01:42.079
So we're going to pass in the value schema as

00:01:42.079 --> 00:01:49.700
purchase and the name of the attribute is schema as Purchase.schema.

00:01:49.700 --> 00:01:51.875
There's one other thing we have to do.

00:01:51.875 --> 00:01:56.090
You'll notice that the function we were previously using to actually configure

00:01:56.090 --> 00:02:01.060
the schema and serialize it in the purchase object is gone.

00:02:01.060 --> 00:02:05.629
That's because avro in confluent kafka-python library

00:02:05.629 --> 00:02:10.460
has utility function that we can use to actually package the avro schema for us.

00:02:10.460 --> 00:02:15.040
So this is now just a simple string as you can see,

00:02:15.039 --> 00:02:17.789
and we're just going to call avro.loads.

00:02:17.789 --> 00:02:20.549
So this says, use the avro functionality from

00:02:20.550 --> 00:02:25.265
the confluent kafka-python library and load the string definition

00:02:25.264 --> 00:02:32.839
of our schema into this object and now we have that our producers should be all set.

00:02:32.840 --> 00:02:36.920
We're going to define the value schema as purchase dot schema.

00:02:36.919 --> 00:02:39.979
So let's go ahead and just run this as is we're not

00:02:39.979 --> 00:02:42.784
quite done yet but let's see if our producer works.

00:02:42.784 --> 00:02:45.185
So our consumers not doing anything.

00:02:45.185 --> 00:02:48.770
So we're going to go ahead and exit or it is getting the data but as you can see,

00:02:48.770 --> 00:02:50.060
it's still in binary format.

00:02:50.060 --> 00:02:51.965
So this isn't very useful to us.

00:02:51.965 --> 00:02:56.300
We can't really tell what it is because we have all this binary compacted data.

00:02:56.300 --> 00:03:02.314
So let's exit out here and let's now update our consumer to also use the schema registry.

00:03:02.314 --> 00:03:06.560
So again we're going to use that variable,

00:03:06.560 --> 00:03:11.689
schema registry URL, to instantiate a new schema registry type.

00:03:11.689 --> 00:03:15.020
We're going to update this to be an AvroConsumer instead of

00:03:15.020 --> 00:03:18.094
just a normal consumer and we're going to specify

00:03:18.094 --> 00:03:21.995
schema registry instance to the one we just constructed,

00:03:21.995 --> 00:03:25.145
and then that should be just about it.

00:03:25.145 --> 00:03:27.605
That's all the updating we really need to do at the consumer.

00:03:27.604 --> 00:03:31.369
So let's run our application one more time and see how it functions.

00:03:31.370 --> 00:03:33.110
As you can see now,

00:03:33.110 --> 00:03:36.290
we can actually read and see that the data looks much nicer and

00:03:36.289 --> 00:03:40.025
easier to understand just like normal JSON would've seen historically.

00:03:40.025 --> 00:03:43.370
So you can see that using the AvroConsumer and producer and

00:03:43.370 --> 00:03:46.335
the built-in avro functionality in the

00:03:46.335 --> 00:03:49.370
kafka confluent python library can really make

00:03:49.370 --> 00:03:52.925
it much easier to use avro than to do it yourself manually.

00:03:52.925 --> 00:03:55.925
It's also worth mentioning while we are here that

00:03:55.925 --> 00:03:58.850
the usage of schema registry is also nice because we didn't have

00:03:58.849 --> 00:04:01.444
to actually package our avro schema definition

00:04:01.444 --> 00:04:04.930
alongside the messages that we were producing or consuming.

00:04:04.930 --> 00:04:07.760
The library took care of fetching the schema for us and

00:04:07.759 --> 00:04:11.579
shepherding the data into and out of the avro format.

