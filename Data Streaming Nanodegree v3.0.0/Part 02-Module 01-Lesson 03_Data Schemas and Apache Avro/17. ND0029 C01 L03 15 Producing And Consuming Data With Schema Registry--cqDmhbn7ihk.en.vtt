WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.259
Sending our Avro schema definition alongside every message introduces

00:00:04.259 --> 00:00:08.654
some additional network and storage overhead in our producer and consumer applications.

00:00:08.654 --> 00:00:12.149
Not only that, but it introduces additional work on the consumer

00:00:12.150 --> 00:00:15.990
and producer to correctly serialize and deserialize from Avro.

00:00:15.990 --> 00:00:20.399
Schema Registry is a tool built by confluent and deployed alongside

00:00:20.399 --> 00:00:25.109
Apache Kafka that can help reduce some of the overhead involved with using Avro.

00:00:25.109 --> 00:00:27.794
Schema Registry is in use in your cluster.

00:00:27.795 --> 00:00:31.515
You no longer need to send schemas alongside your payloads to Kafka.

00:00:31.515 --> 00:00:34.740
Instead, your Kafka client can be configured to send

00:00:34.740 --> 00:00:38.825
the schema to Schema Registry over HTTP instead.

00:00:38.825 --> 00:00:41.900
Schema Registry assigns the name schema

00:00:41.899 --> 00:00:45.289
a version number and then stores it in a private topic.

00:00:45.289 --> 00:00:48.950
Until the schema definition is updated or changes again,

00:00:48.950 --> 00:00:51.080
the producer never needs to send a schema to

00:00:51.079 --> 00:00:54.905
either Scheme Registry or the Kafka brokers ever again.

00:00:54.905 --> 00:00:58.700
When consumers then consume data from this Kafka topic,

00:00:58.700 --> 00:01:01.400
the Kafka client library will automatically retrieve

00:01:01.399 --> 00:01:04.400
the average schema for the message from Schema Registry.

00:01:04.400 --> 00:01:07.594
Schema Registry can pull historical schemas as well.

00:01:07.594 --> 00:01:11.885
So all data stored in the Kafka topic can be deserialized by clients.

00:01:11.885 --> 00:01:15.500
As soon as the client has retrieved a specific version of a schema,

00:01:15.500 --> 00:01:18.905
it does not need to retrieve it from Schema Registry ever again.

00:01:18.905 --> 00:01:22.415
Just to emphasize the point when using Schema Registry,

00:01:22.415 --> 00:01:28.175
consumers and producers only fetch or produce a schema when they don't already have it.

00:01:28.174 --> 00:01:30.155
Once they fetch to schema version,

00:01:30.155 --> 00:01:31.805
it's never fetched again.

00:01:31.805 --> 00:01:36.050
This can dramatically decrease networking overhead for high throughput applications.

00:01:36.049 --> 00:01:39.784
It's worth mentioning that Schema Registry does not support deletes by default.

00:01:39.784 --> 00:01:43.954
You can enable us behavior but most Schema Registry brokers don't have this enabled.

00:01:43.954 --> 00:01:47.245
I don't recommend using it if you're not already.

00:01:47.245 --> 00:01:51.050
In confluent, Kafka Python library using in this course has native support for

00:01:51.049 --> 00:01:53.599
Schema Registry as do many client libraries in

00:01:53.599 --> 00:01:56.539
Java Go and other popular programming languages.

00:01:56.540 --> 00:02:00.410
If for some reason your client library doesn't support Schema Registry natively,

00:02:00.409 --> 00:02:04.670
you can still use it by integrating against its HTTP API.

00:02:04.670 --> 00:02:08.719
Finally, Schema Registry can be used by any application that wants to

00:02:08.719 --> 00:02:12.784
efficiently store and retrieve schema data across multiple versions.

00:02:12.784 --> 00:02:16.280
Just to reiterate, Schema Registry isn't specific to Kafka.

00:02:16.280 --> 00:02:19.444
It's just that it was built originally for a Kafka use case.

00:02:19.444 --> 00:02:21.289
If you'd like to features and functionality of

00:02:21.289 --> 00:02:23.750
Schema Registry and it's already available at your company,

00:02:23.750 --> 00:02:28.110
you can use it in any application whether or not it's interacting with Kafka.

