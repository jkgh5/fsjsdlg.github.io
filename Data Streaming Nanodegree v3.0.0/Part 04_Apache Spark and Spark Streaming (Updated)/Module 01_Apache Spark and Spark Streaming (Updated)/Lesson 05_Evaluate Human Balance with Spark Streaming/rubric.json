{
  "id": 2895,
  "project_id": 768,
  "upload_types": [
    "repo",
    "zip"
  ],
  "file_filter_regex": "\\A(?!(((.*/)?(__MACOSX|\\.git|node_modules|bower_components|jspm_packages|\\.idea|build|.ipynb_checkpoints|\\.Trash-0|logs)(\\Z|/))))((.*\\.(js|css|py|html|htm|txt|md|markdown|sql|swift|java|ts|gradle|xml|rst|yml|yaml|rmd|pdf|docx|h|H|hh|hxx|h\\+\\+|c|C|cc|cpp|cxx|c\\+\\+)\\Z)|((.*/)?(README|Readme|readme|Makefile)\\Z))",
  "nomination_eligible": false,
  "stand_out": "1. Create a streaming spark application that listens to the **redis-server** kafka topic and analyzes the Redis SortedSet called RapidStepTest to calculate a risk score (see pseudocode below) based on the last 4 step tests for each customer. The goal is that the risk score reflects an integer which is negative when the step time is going down, and positive when the risk score is going up.\n2. Create a streaming spark application that listens to the redis-server kafka topic, and analyzes the Redis SortedSet called RapidStepTest to calculate a risk score (see pseudocode below) based on the last 4 step tests for each customer. Also listen to the **stedi-events** kafka topic for the risk score emitted by the STEDI application. Compare the scores side by side in a sink to the console and evaluate whether the score is correct or not. The goal is that the risk score reflects an integer which is negative when the step time is going down, and positive when the risk score is going up.\n\n```\npublic static CustomerRisk riskScore(String email) throws Exception{\n\n    List<RapidStepTest> rapidStepTestsSortedByDate = allTests.stream().filter(historicUserPredicate).sorted(Comparator.comparing(RapidStepTest::getStartTime)).collect(Collectors.toList());\n    if (rapidStepTestsSortedByDate.size()<4){\n        throw new Exception(\"Customer \"+email+\" has: \"+rapidStepTestsSortedByDate.size()+\" rapid step tests on file which is less than the required number(4) to calculate fall risk.\");\n    }\n\n    BigDecimal currentTestAverageScore = BigDecimal.valueOf((mostRecentTest.getStopTime()-mostRecentTest.getStartTime())+ (secondMostRecentTest.getStopTime()-secondMostRecentTest.getStartTime())).divide(BigDecimal.valueOf(2l));\n\n    BigDecimal previousTestAverageScore = BigDecimal.valueOf((thirdMostRecentTest.getStopTime()-thirdMostRecentTest.getStartTime())+ (fourthMostRecentTest.getStopTime()-fourthMostRecentTest.getStartTime())).divide(BigDecimal.valueOf(2l));\n\n    BigDecimal riskScore = (previousTestAverageScore.subtract(currentTestAverageScore)).divide(new BigDecimal(1000l));\n\n    CustomerRisk customerRisk = new CustomerRisk();\n    customerRisk.setScore(new Float(riskScore.setScale(2, BigDecimal.ROUND_HALF_UP).toString()));\n    customerRisk.setCustomer(email);\n    customerRisk.setRiskDate(new Date(mostRecentTest.getStopTime()));\n\n    return customerRisk;\n}\n```",
  "hide_criteria": false,
  "created_at": "2020-08-14T20:51:38.012Z",
  "updated_at": "2021-01-29T04:08:12.822Z",
  "hashtag": "",
  "max_upload_size_mb": 500,
  "estimated_sla": null,
  "project_assistant_enabled": false,
  "available_for_cert_project": false,
  "language": "en-us",
  "ndkeys": [
    "nd029-beta",
    "nd029",
    "nd029-ent"
  ],
  "coursekeys": [],
  "is_career": false,
  "sections": [
    {
      "id": 6299,
      "name": "Initialize Spark Components",
      "created_at": "2020-09-21T17:24:03.091Z",
      "updated_at": "2020-09-21T17:24:08.444Z",
      "deleted_at": null,
      "position": 0,
      "rubric_id": 2895,
      "rubric_items": [
        {
          "id": 17989,
          "section_id": 6299,
          "passed_description": "The log `/home/workspace/spark/logs/spark-[username]-org.apache.spark.deploy.worker.Worker-1-[hostname].out` contains a statement “Successfully registered with master spark”, like this:\n\n```\n20/09/12 22:19:15 INFO Worker: Successfully registered with master spark://Seans-MBP:7077\n```",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:25:22.241Z",
          "updated_at": "2020-09-21T17:31:11.803Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "Start a Spark cluster",
          "exceedable": false
        },
        {
          "id": 17991,
          "section_id": 6299,
          "passed_description": "The log `/home/workspace/spark/logs/kafkajoin.log` does not have any Python, Java, or Scala errors.",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:35:32.384Z",
          "updated_at": "2020-09-21T17:37:09.253Z",
          "deleted_at": null,
          "optional": false,
          "position": 2,
          "criteria": "The Spark application successfully executes on the Spark cluster",
          "exceedable": false
        }
      ]
    },
    {
      "id": 6300,
      "name": "Continuous Application",
      "created_at": "2020-09-21T17:24:08.596Z",
      "updated_at": "2020-09-21T17:25:22.027Z",
      "deleted_at": null,
      "position": 1,
      "rubric_id": 2895,
      "rubric_items": [
        {
          "id": 17992,
          "section_id": 6300,
          "passed_description": "As evidence, provide at least two screenshots of the STEDI risk graph showing a change in the data captured. The graph should have different data points.",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:37:09.477Z",
          "updated_at": "2020-09-21T18:32:49.319Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "The data pipeline provides continuous output of customer risk scores",
          "exceedable": false
        },
        {
          "id": 17993,
          "section_id": 6300,
          "passed_description": "The sink statement at the end of the sparkpykafkajoin.py script uses the proper method to ensure that the application runs continuously (until either killed or the computer shuts down).",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:40:10.204Z",
          "updated_at": "2020-09-21T17:42:02.550Z",
          "deleted_at": null,
          "optional": false,
          "position": 1,
          "criteria": "The spark streaming application doesn’t exit prematurely",
          "exceedable": false
        }
      ]
    },
    {
      "id": 6301,
      "name": "Consume and Process Data",
      "created_at": "2020-09-21T17:24:09.864Z",
      "updated_at": "2020-09-21T17:25:22.033Z",
      "deleted_at": null,
      "position": 2,
      "rubric_id": 2895,
      "rubric_items": [
        {
          "id": 17994,
          "section_id": 6301,
          "passed_description": "A streaming dataframe is instantiated in the `sparkpykafkajoin.py` script with the **redis-server** Kafka topic as a source.",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:42:02.694Z",
          "updated_at": "2020-09-21T18:23:01.677Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "The spark streaming application consumes data from the **redis-server** Kafka topic",
          "exceedable": false
        },
        {
          "id": 17995,
          "section_id": 6301,
          "passed_description": "A streaming dataframe is instantiated in the `sparkpykafkajoin.py` script with the **stedi-events** Kafka topic as a source.",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:44:27.370Z",
          "updated_at": "2020-09-21T18:23:01.681Z",
          "deleted_at": null,
          "optional": false,
          "position": 1,
          "criteria": "The spark streaming application consumes data from the **stedi-events** Kafka topic",
          "exceedable": false
        },
        {
          "id": 17996,
          "section_id": 6301,
          "passed_description": "The Python script `sparkpykafkajoin.py` properly invokes the appropriate function to parse JSON and decode Base64 encoded data from Kafka.",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:46:15.213Z",
          "updated_at": "2020-09-21T18:24:12.130Z",
          "deleted_at": null,
          "optional": false,
          "position": 2,
          "criteria": "The spark streaming application parses JSON and Base64 decodes (where applicable) the data received from Kafka",
          "exceedable": false
        }
      ]
    },
    {
      "id": 6302,
      "name": "Aggregation of Streaming Dataframes",
      "created_at": "2020-09-21T17:24:11.078Z",
      "updated_at": "2020-09-21T17:25:22.036Z",
      "deleted_at": null,
      "position": 3,
      "rubric_id": 2895,
      "rubric_items": [
        {
          "id": 17997,
          "section_id": 6302,
          "passed_description": "The two distinct streaming dataframes joined for the aggregate streaming dataframes contain the following:\n**emailAndBirthYear**: Email and Birth Year\n**emailAndRiskScore**: Email and Risk Score",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:48:30.572Z",
          "updated_at": "2020-09-21T18:00:59.291Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "The Spark streaming application instantiates two distinct streaming dataframes",
          "exceedable": false
        },
        {
          "id": 17998,
          "section_id": 6302,
          "passed_description": "In the Python script `sparkpykafkajoin.py`, the streaming dataframe on which `.join()` is invoked contains information about either the customer risk or the customer record. The streaming dataframe passed inside the `.join()` function call contains information about either the customer risk or the customer record including the birth year. \n\nThe Python script `sparkpykafkajoin.py` calls the appropriate function to join two separate dataframes to create **riskScoreByBirthYear**.",
          "exceeded_description": null,
          "created_at": "2020-09-21T17:51:26.384Z",
          "updated_at": "2020-09-21T18:00:59.295Z",
          "deleted_at": null,
          "optional": false,
          "position": 1,
          "criteria": "The Spark streaming application joins the two streaming dataframes **emailAndBirthYear** and **emailAndRiskScore** to create **riskScoreByBirthYear**",
          "exceedable": false
        }
      ]
    },
    {
      "id": 6303,
      "name": "Sink Datastream",
      "created_at": "2020-09-21T17:24:11.781Z",
      "updated_at": "2020-09-21T17:25:22.039Z",
      "deleted_at": null,
      "position": 4,
      "rubric_id": 2895,
      "rubric_items": [
        {
          "id": 18000,
          "section_id": 6303,
          "passed_description": "In the Python script `sparkpykafkajoin.py`, the application sinks a streaming dataframe to a Kafka topic which is defined in the `/home/workspace/stedi-application/application.conf` file (example below):\n\n```\nkafka{\n  riskTopic=customer-risk\n}\n```",
          "exceeded_description": null,
          "created_at": "2020-09-21T18:01:10.563Z",
          "updated_at": "2020-09-21T18:26:08.642Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "The spark streaming application produces a sink to Kafka",
          "exceedable": false
        },
        {
          "id": 18001,
          "section_id": 6303,
          "passed_description": "In the Python script `sparkpykafkajoin.py`, the appropriate function is called to sink the **riskScoreByBirthYear** streaming dataframe to Kafka in JSON format below:\n\n```\n{\n\"customer\":\"Santosh.Fibonnaci@test.com\",\n\"Score\":\"28.5\",\n\"email\":\"Santosh.Fibonnaci@test.com\",\n\"birthYear\":\"1963\"\n} \n```",
          "exceeded_description": null,
          "created_at": "2020-09-21T18:03:55.821Z",
          "updated_at": "2020-09-21T18:26:08.647Z",
          "deleted_at": null,
          "optional": false,
          "position": 1,
          "criteria": "The streaming dataframe being sinked with Kafka contains JSON formatted data",
          "exceedable": false
        }
      ]
    },
    {
      "id": 6304,
      "name": "Validation",
      "created_at": "2020-09-21T17:24:12.631Z",
      "updated_at": "2020-09-21T17:25:22.041Z",
      "deleted_at": null,
      "position": 5,
      "rubric_id": 2895,
      "rubric_items": [
        {
          "id": 18002,
          "section_id": 6304,
          "passed_description": "The Python script `sparkpyeventskafkastreamtoconsole.py` sinks a streaming dataframe to the console which contains customer risk information in a human-readable format. (Example below):\n\n```\n+--------------------+-----\n|customer           |score|\n+--------------------+-----+\n|Spencer.Davis@tes...| 8.0|\n+--------------------+-----\n```",
          "exceeded_description": null,
          "created_at": "2020-09-21T18:09:39.241Z",
          "updated_at": "2020-09-21T18:18:40.298Z",
          "deleted_at": null,
          "optional": false,
          "position": 0,
          "criteria": "Outputs a log which contains customer risk information in a console output format",
          "exceedable": false
        },
        {
          "id": 18003,
          "section_id": 6304,
          "passed_description": "The Python script `sparkpyrediskafkastreamtoconsole.py` sinks a streaming dataframe to the console which contains customer birth year information in a human-readable format. (Example below):\n\n```\n+--------------------+-----               \n| email         |birthYear|\n+--------------------+-----\n|Gail.Spencer@test...|1963|\n|Craig.Lincoln@tes...|1962|\n|  Edward.Wu@test.com|1961|\n|Santosh.Phillips@...|1960|\n|Sarah.Lincoln@tes...|1959|\n|Sean.Howard@test.com|1958|\n|Sarah.Clark@test.com|1957|\n+--------------------+-----\n```",
          "exceeded_description": null,
          "created_at": "2020-09-21T18:10:53.399Z",
          "updated_at": "2020-09-21T18:18:40.301Z",
          "deleted_at": null,
          "optional": false,
          "position": 1,
          "criteria": "Outputs a log which contains customer birth year information in a console output format",
          "exceedable": false
        }
      ]
    }
  ],
  "project": {
    "id": 768,
    "name": "Evaluate Human Balance with Spark Streaming",
    "nanodegree_key": "nd029",
    "is_cert_project": false,
    "audit_project_id": null,
    "hashtag": null,
    "audit_rubric_id": null,
    "entitlement_required": false,
    "is_career": false,
    "recruitment_family_id": 5,
    "created_at": "2020-09-23T22:15:57.917Z",
    "updated_at": "2021-04-06T01:08:13.898Z",
    "price": "7.0",
    "ungradeable_price": "3.0",
    "audit_price": "0.0"
  }
}