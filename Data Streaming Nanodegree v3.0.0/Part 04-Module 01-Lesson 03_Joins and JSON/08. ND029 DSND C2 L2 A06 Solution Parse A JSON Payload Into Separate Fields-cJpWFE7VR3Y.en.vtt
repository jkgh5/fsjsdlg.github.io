WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.700
Let's look at the solution for parsing a JSON payload into separate fields.

00:00:05.700 --> 00:00:08.745
The first thing I'm going to do is make sure that Kafka is running.

00:00:08.745 --> 00:00:10.680
I'm going to type ps-ef,

00:00:10.680 --> 00:00:12.795
ensuring up it is not running.

00:00:12.795 --> 00:00:15.404
I'm going to click "Start Kafka."

00:00:15.404 --> 00:00:18.570
I'm going to close this.

00:00:18.570 --> 00:00:22.295
I'm going to make this a little easier to work with.

00:00:22.295 --> 00:00:24.860
I just scroll down to the last thing,

00:00:24.860 --> 00:00:27.095
the terminals saying it's still starting Kafka.

00:00:27.095 --> 00:00:32.210
I'm going to start up Spark because I didn't see that running either. No spark here.

00:00:32.210 --> 00:00:37.860
I'm going to go to home/workspace/spark/sbin lists

00:00:37.860 --> 00:00:41.310
that and I see start master and start slave scripts.

00:00:41.310 --> 00:00:47.115
I'm going to say start-master.sh with dot slash, that means here.

00:00:47.115 --> 00:00:49.935
I've now got the log file.

00:00:49.935 --> 00:00:52.095
I'm going to cut that,

00:00:52.095 --> 00:00:54.510
and I'm going to cut it one more time,

00:00:54.510 --> 00:00:58.230
and there is my spark URI for the master.

00:00:58.230 --> 00:01:00.440
I've got my other script.

00:01:00.440 --> 00:01:02.150
I'm going to use to start the worker,

00:01:02.150 --> 00:01:05.465
and I'm going to pass the URI of the master to it.

00:01:05.465 --> 00:01:07.465
That's started up Spark.

00:01:07.465 --> 00:01:09.010
If I say ps-ef,

00:01:09.010 --> 00:01:12.815
now I see Spark once and twice,

00:01:12.815 --> 00:01:17.130
and it looks like Kafka is almost done as well. Yes, it is.

00:01:17.130 --> 00:01:22.655
Now I can start coding and I'm ready to submit my application as soon as it's done.

00:01:22.655 --> 00:01:26.330
The first thing I want to do is look at the JSON format that I'll be

00:01:26.330 --> 00:01:30.450
parsing and create a dictionary or a schema for it.

00:01:30.710 --> 00:01:36.555
This kind of data is actually for bank deposits.

00:01:36.555 --> 00:01:40.700
So I'm going to create a schema called bankDepositSchema,

00:01:40.700 --> 00:01:43.475
and that's a StructType,

00:01:43.475 --> 00:01:48.680
and that's a Python class we use to create schemas.

00:01:48.680 --> 00:01:53.240
We use an array of StructFields to do that,

00:01:53.240 --> 00:01:58.810
and they need to match each of these field names exactly and the types.

00:01:58.810 --> 00:02:03.620
The first one is account number and what's the type?

00:02:03.620 --> 00:02:05.765
It's got quotes around it,

00:02:05.765 --> 00:02:07.160
and so because of that,

00:02:07.160 --> 00:02:09.545
we need it to be called a string.

00:02:09.545 --> 00:02:12.250
How about the amount?

00:02:12.250 --> 00:02:14.505
That doesn't have quotes.

00:02:14.505 --> 00:02:17.310
It's not a string, it's actually at float.

00:02:17.310 --> 00:02:22.055
So we're going to import flow type because we didn't have that imported,

00:02:22.055 --> 00:02:25.405
and we're going to call it FloatType.

00:02:25.405 --> 00:02:29.765
DateAndTime is surrounded by quotes as well.

00:02:29.765 --> 00:02:33.545
There are some other formats like date that would work.

00:02:33.545 --> 00:02:38.620
We're not going to be using the date for really anything other than display purposes,

00:02:38.620 --> 00:02:41.420
so for the purpose of this exercise,

00:02:41.420 --> 00:02:42.785
we'll just call it a string,

00:02:42.785 --> 00:02:44.715
which will work fine.

00:02:44.715 --> 00:02:50.680
Now we've got the schema and we're ready to create a SparkSession.

00:02:50.680 --> 00:02:56.555
The name of the topic needs to be an exact match but the application name doesn't.

00:02:56.555 --> 00:02:58.885
I usually make them the same,

00:02:58.885 --> 00:03:00.420
that doesn't always work.

00:03:00.420 --> 00:03:03.755
You might have a Spark application that reads lots of topics.

00:03:03.755 --> 00:03:06.590
You might want to just think of a name that describes what it's

00:03:06.590 --> 00:03:10.340
doing or setting the log level to what?

00:03:10.340 --> 00:03:13.880
To WARN. That's lower than info,

00:03:13.880 --> 00:03:14.990
which is the default.

00:03:14.990 --> 00:03:17.180
I don't like info because it gives a lot of

00:03:17.180 --> 00:03:20.240
data and it's hard to find what you're looking for.

00:03:20.240 --> 00:03:23.540
Now let's read from the Kafka topic.

00:03:23.540 --> 00:03:27.470
We're going to call the dataframe that we get,

00:03:27.470 --> 00:03:35.880
bankDepositsRawStreamingDF for dataframe, and we're going to call it readStream.format.

00:03:35.880 --> 00:03:42.045
What's the format? Kafka. What are the options we need? What's the first one?

00:03:42.045 --> 00:03:45.850
Kafka.bootstrap.servers.

00:03:47.890 --> 00:03:54.140
The first server and the only one in our case is localhost:9092,

00:03:54.140 --> 00:03:55.685
that's the port number.

00:03:55.685 --> 00:03:58.725
What else do we need to tell the topic?

00:03:58.725 --> 00:04:01.589
They call that the subscribe option,

00:04:01.589 --> 00:04:06.830
and that does need to be an exact match with our topic name.

00:04:06.830 --> 00:04:08.870
We want to get all the data,

00:04:08.870 --> 00:04:10.840
so what will the value be here?

00:04:10.840 --> 00:04:14.270
Earliest. We're going to load,

00:04:14.270 --> 00:04:19.920
and that'll load the data from that topic into a dataframe.

00:04:19.920 --> 00:04:24.910
We're going to take the binary format that we get and convert it to non-binary.

00:04:24.910 --> 00:04:30.250
We'll create another dataframe to hold it called bankDepositStreamingDF,

00:04:30.250 --> 00:04:34.355
and we're going to refer to the raw data

00:04:34.355 --> 00:04:39.979
and run an expression against it to change some things.

00:04:39.979 --> 00:04:45.435
We want to cast the key as a string instead of binary,

00:04:45.435 --> 00:04:48.870
and the key is the account number.

00:04:48.870 --> 00:04:53.620
We're going to cast the value as a string as well,

00:04:53.620 --> 00:04:58.040
and the value in this case is the bankDepositJson.

00:05:00.960 --> 00:05:06.710
Now we've got our schema and we've got the JSON.

00:05:06.710 --> 00:05:10.685
Let's have fun with that.

00:05:10.685 --> 00:05:14.125
We're going to take the JSON and use the schema with it.

00:05:14.125 --> 00:05:17.400
Make it some meaningful fields.

00:05:17.400 --> 00:05:25.165
I'm going to go ahead and take the streaming dataframe and call.withColumn.

00:05:25.165 --> 00:05:32.645
The column I'm acting on that I'm actually going to replace is bankDepositJson.

00:05:32.645 --> 00:05:38.225
Then I'm going to call from JSON to populate that field, I'm overriding it.

00:05:38.225 --> 00:05:40.820
Where is the JSON coming from?

00:05:40.820 --> 00:05:44.265
The bankDepositJson field.

00:05:44.265 --> 00:05:46.440
Where is the schema coming from?

00:05:46.440 --> 00:05:48.785
BankDepositSchema.

00:05:48.785 --> 00:05:54.175
Now I want to select the nested fields that are created as a result.

00:05:54.175 --> 00:06:01.910
I'm going to use the call function and I'm going to select bankDepositJson.star.

00:06:02.240 --> 00:06:08.160
That gives me another dataframe that has the field selected like this,

00:06:08.160 --> 00:06:12.325
as separate fields with the correct names.

00:06:12.325 --> 00:06:14.660
I want save that as a view.

00:06:14.660 --> 00:06:17.925
I'm going to createOrReplaceTempView,

00:06:17.925 --> 00:06:21.140
and let's call the view BankDeposits.

00:06:21.140 --> 00:06:25.360
Now I'm going to select from that and let's look four things where

00:06:25.360 --> 00:06:30.030
the amount is, say, over $200.

00:06:30.030 --> 00:06:34.140
We're going to use SQL to do that.

00:06:34.140 --> 00:06:43.590
I'm going to say bankDepositsSelectDF equals spark.sql.

00:06:43.590 --> 00:06:44.760
What would be the query?

00:06:44.760 --> 00:06:45.920
This is the view,

00:06:45.920 --> 00:06:47.015
this is the field,

00:06:47.015 --> 00:06:49.675
and I want it to be greater than 200.

00:06:49.675 --> 00:06:56.615
I'm going to select all the fields that I want from the view called BankDeposits,

00:06:56.615 --> 00:07:01.255
where amount is greater than 200.

00:07:01.255 --> 00:07:07.220
Now I want to take those results and write them to the console.

00:07:07.220 --> 00:07:16.890
I'm going to write bankDepositsSelectDF.writeStream.outputMode append.

00:07:16.890 --> 00:07:21.155
Format is console, so I can see it.

00:07:21.155 --> 00:07:23.090
Tell it to start,

00:07:23.090 --> 00:07:28.015
and then tell it to wait to stop till it's told to.

00:07:28.015 --> 00:07:32.850
I'm going to clear this terminal to make it easier to read.

00:07:32.990 --> 00:07:38.480
This is actually a script that stored under homework space.

00:07:38.480 --> 00:07:41.030
There's a script to run this Python file.

00:07:41.030 --> 00:07:45.785
So if I ls, I can see that it's the submit-bank-deposits.sh.

00:07:45.785 --> 00:07:51.730
I'm going to say./submit-bank-deposits-sh.

00:07:51.730 --> 00:07:57.895
This only works if I first cd/home/workspace.

00:07:57.895 --> 00:08:01.900
Now it's found all the Java libraries it needs,

00:08:01.900 --> 00:08:05.650
and now it's running the Spark application.

00:08:05.650 --> 00:08:11.315
We're waiting to see the deposits greater than $200.

00:08:11.315 --> 00:08:14.585
This part can take a minute or two,

00:08:14.585 --> 00:08:16.170
and there's the data.

00:08:16.170 --> 00:08:18.105
Now let's look at the amounts.

00:08:18.105 --> 00:08:20.280
They are all over 200.

00:08:20.280 --> 00:08:23.280
The Spark SQL is working correctly.

00:08:23.280 --> 00:08:26.175
Congratulations if you got that working,

00:08:26.175 --> 00:08:28.505
and if you're still working on it, that's okay.

00:08:28.505 --> 00:08:30.350
We're going to learn this together.

00:08:30.350 --> 00:08:34.440
I'm going kill this by seeing Control C.

