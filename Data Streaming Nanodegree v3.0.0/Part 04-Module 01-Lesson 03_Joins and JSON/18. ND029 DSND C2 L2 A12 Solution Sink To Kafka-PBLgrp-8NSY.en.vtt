WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.210
Let's look at one way of solving the exercise.

00:00:03.210 --> 00:00:07.200
The first thing I'm going to do is make sure that Kafka and Spark are running.

00:00:07.200 --> 00:00:11.415
I don't think they are, because I just came into the workspace for the first time today.

00:00:11.415 --> 00:00:15.135
So sure enough if I do PS - EF, they're not running.

00:00:15.135 --> 00:00:18.930
So I'm going to click this start Kafka button and get that going.

00:00:18.930 --> 00:00:22.275
Can I make this a little easier to work with here?

00:00:22.275 --> 00:00:25.275
While that's running, let's go start up Spark.

00:00:25.275 --> 00:00:30.675
So now I'm going to go to the home workspace,

00:00:30.675 --> 00:00:34.770
Spark folder, and then I'm going go to the sbin folder.

00:00:34.770 --> 00:00:39.450
I'm now in home/workspace/spark/sbin.

00:00:39.450 --> 00:00:41.770
I am going to say,./ start-master.sh.

00:00:42.070 --> 00:00:46.565
Copy the log directory and file name,

00:00:46.565 --> 00:00:51.730
then I'm going to tail that with tail-f and the filename.

00:00:51.730 --> 00:00:55.975
I'm waiting for the path to the spark-master.

00:00:55.975 --> 00:00:59.450
Still waiting, haven't seen it yet.

00:00:59.450 --> 00:01:03.155
Sometimes Spark Masters are a little slower to start.

00:01:03.155 --> 00:01:06.745
There it is, copying that.

00:01:06.745 --> 00:01:09.320
I can kill the tail without killing

00:01:09.320 --> 00:01:12.665
the process tail is just watching the log file so that doesn't hurt anything.

00:01:12.665 --> 00:01:14.915
So "Control C" gets me out of that.

00:01:14.915 --> 00:01:23.610
Now I'm going to do do./start-slave.sh with the Master UI. It's running.

00:01:23.610 --> 00:01:29.360
Let's go back over here to Kafka and we have a blinking cursor,

00:01:29.360 --> 00:01:31.580
and this is a normal error.

00:01:31.580 --> 00:01:33.920
So we've got everything running.

00:01:33.920 --> 00:01:36.200
Now we're ready to write code.

00:01:36.200 --> 00:01:38.780
I've got a bunch of imports here.

00:01:38.780 --> 00:01:42.665
It's important to remember that when you write these on your own, those won't be there.

00:01:42.665 --> 00:01:45.740
When you're writing a Spark application,

00:01:45.740 --> 00:01:49.340
be sure to import the proper libraries that you need.

00:01:49.340 --> 00:01:55.205
I've got a payload here that's coming from bank withdrawals topic,

00:01:55.205 --> 00:01:57.380
and if I look at the data that I have,

00:01:57.380 --> 00:01:59.000
I have an accountNumber,

00:01:59.000 --> 00:02:00.304
I have an amount,

00:02:00.304 --> 00:02:01.955
I have a dateAndTime,

00:02:01.955 --> 00:02:04.745
and I have a transactionId.

00:02:04.745 --> 00:02:10.580
I also have an atm withdrawals message coming from another Kafka topic,

00:02:10.580 --> 00:02:13.270
and it includes the transaction date,

00:02:13.270 --> 00:02:17.305
the transactionId, and the atmLocation.

00:02:17.305 --> 00:02:20.090
I need to create a schema for both of

00:02:20.090 --> 00:02:23.885
these in order to pass the JSON data that I'm going to get.

00:02:23.885 --> 00:02:29.180
So I'm going to start by creating the schema for the BankWithdrawals data.

00:02:29.180 --> 00:02:33.570
Call it BankWithdrawalsMessageSchema.

00:02:35.990 --> 00:02:42.840
It's going to be a StruckType and I'm going to pass it in an array of StruckFields.

00:02:42.840 --> 00:02:47.039
Each field needs to match the JSON field exactly,

00:02:47.039 --> 00:02:51.270
and again we're doing this because we're 4Spark3

00:02:51.270 --> 00:02:56.190
Spark 3.0 does automatic JSON field inference,

00:02:56.190 --> 00:02:58.345
but because we're on 2.4.6,

00:02:58.345 --> 00:03:00.320
we're not going to have that available.

00:03:00.320 --> 00:03:02.000
So accountNumber is a string,

00:03:02.000 --> 00:03:04.930
we know that because it has quotes around the value.

00:03:04.930 --> 00:03:10.205
Now we're going do an amount and I noticed that I don't have a float imported.

00:03:10.205 --> 00:03:12.380
So I'm going to do a float type,

00:03:12.380 --> 00:03:15.365
and make sure I import that before I use it.

00:03:15.365 --> 00:03:17.015
It wouldn't cause an error yet,

00:03:17.015 --> 00:03:19.360
but when I try to run it, it would.

00:03:19.360 --> 00:03:23.025
Now I'm going to create the fields for dateAndTime,

00:03:23.025 --> 00:03:28.245
and we're going to make it a string right now. What am I missing?

00:03:28.245 --> 00:03:32.460
TransactionId, and it looks like that's an integer.

00:03:32.460 --> 00:03:35.070
There's no quotes around the value.

00:03:35.070 --> 00:03:38.614
So I could make that an integer type.

00:03:38.614 --> 00:03:41.125
I could also make it a string.

00:03:41.125 --> 00:03:45.545
Now I'm looking at this other data that has a transactionId as well.

00:03:45.545 --> 00:03:47.300
We're going to join on that field,

00:03:47.300 --> 00:03:48.680
and it's an integer.

00:03:48.680 --> 00:03:51.920
So I think if we make them both integers, we should be okay.

00:03:51.920 --> 00:03:54.050
Now I'm going to make this schema for

00:03:54.050 --> 00:03:59.050
the other payload called atmWithdrawalsMessageSchema.

00:04:00.860 --> 00:04:04.950
I'm going to pass the StruckFields that I need.

00:04:04.950 --> 00:04:07.620
The first one is the transactionDate,

00:04:07.620 --> 00:04:10.945
and again we're going to treat it as a string.

00:04:10.945 --> 00:04:13.910
If I needed to calculate when something

00:04:13.910 --> 00:04:16.820
happened and if it was before or after something else,

00:04:16.820 --> 00:04:18.890
then a string wouldn't help me very much,

00:04:18.890 --> 00:04:20.030
but for this purpose,

00:04:20.030 --> 00:04:21.955
it's just for reporting.

00:04:21.955 --> 00:04:24.340
So I'm going to look at the transactionId,

00:04:24.340 --> 00:04:28.925
which we decided was an integer in both cases.

00:04:28.925 --> 00:04:32.060
Then I've got an atmLocation,

00:04:32.060 --> 00:04:37.940
which is a Stringk, and that's the last field for the atmWithdrawals message.

00:04:37.940 --> 00:04:40.205
It only has three fields.

00:04:40.205 --> 00:04:47.050
So now I'm going to create a Spark Session and we're going to call it bank withdrawals,

00:04:47.050 --> 00:04:50.120
and make sure to say get or create,

00:04:50.120 --> 00:04:52.990
or you will be disappointed.

00:04:52.990 --> 00:04:58.365
Now let's set our log level to warn.

00:04:58.365 --> 00:05:02.715
It's important to know that it doesn't keep us from seeing console output.

00:05:02.715 --> 00:05:04.550
It's changing the log level,

00:05:04.550 --> 00:05:08.110
which is different for the console output.

00:05:08.110 --> 00:05:09.750
I want to create

00:05:09.750 --> 00:05:13.870
the bankWithdrawalsRawStreamingDF,

00:05:20.420 --> 00:05:23.190
there we go.

00:05:23.190 --> 00:05:30.000
That's going to come from Spark through a connection with Kafka.

00:05:30.170 --> 00:05:34.065
We need to make the format Kafka.

00:05:34.065 --> 00:05:41.170
We need to send all the options to connect to Kafka properly quitting the host.

00:05:41.170 --> 00:05:43.850
Again that would normally be a server,

00:05:43.850 --> 00:05:46.430
not just local host.

00:05:46.430 --> 00:05:49.910
We want to subscribe to the right topic for the BankWithdrawals,

00:05:49.910 --> 00:05:53.570
which is called BankWithdrawals.

00:05:53.570 --> 00:05:59.410
We're going to say starting offsets of earliest to get all the data.

00:05:59.410 --> 00:06:01.215
Then we're going to load,

00:06:01.215 --> 00:06:07.905
which starts all of that data frame once the Spark application starts.

00:06:07.905 --> 00:06:16.560
Let's convert this from binary into another data frame called bankWithdrawalsStreamingDF.

00:06:16.560 --> 00:06:20.320
We're going to call.withColumn method.

00:06:21.800 --> 00:06:25.450
We are going to get,

00:06:25.670 --> 00:06:28.110
Oh I'm jumping ahead,

00:06:28.110 --> 00:06:32.050
we actually want to select the stuff into strings first.

00:06:32.050 --> 00:06:34.310
I was about to try and get the JSON.

00:06:34.310 --> 00:06:36.635
But JSON doesn't work very well directly from

00:06:36.635 --> 00:06:39.530
a completely binary payload that's not going to work,

00:06:39.530 --> 00:06:42.395
needs to be a string first.

00:06:42.395 --> 00:06:45.415
Let's cast (key as a string),

00:06:45.415 --> 00:06:49.600
and we're going to call this transactionId.

00:06:49.970 --> 00:06:53.715
We're going to cast (value as a string),

00:06:53.715 --> 00:07:00.390
and we're going to call this bankWithdrawJSON.

00:07:00.390 --> 00:07:05.160
I try to use camel case even for acronyms to be consistent,

00:07:05.160 --> 00:07:07.455
so that's what I did there.

00:07:07.455 --> 00:07:13.080
Now we can convert it and make sure there's an error on there.

00:07:13.310 --> 00:07:17.270
Let's put a new column in there that just overwrote

00:07:17.270 --> 00:07:22.780
the old one that has all this JSON field split out into separate fields.

00:07:22.780 --> 00:07:26.025
We're going to say,.withColumn,

00:07:26.025 --> 00:07:29.280
and the field that has the JSON is what?

00:07:29.280 --> 00:07:35.655
The one that has the Camel case bankWithdrawJSON.

00:07:35.655 --> 00:07:38.540
The schema we want to use is the first one,

00:07:38.540 --> 00:07:44.120
the bankWithdrawalsMessageSchema and out of those I want to select all of the fields.

00:07:44.120 --> 00:07:48.380
I'm going to say column because they're all nested inside of one,

00:07:48.380 --> 00:07:52.800
I can do it like this bankWithdrawJSON.*.

00:07:54.890 --> 00:08:01.100
I want to put those separated fields into a view side-by-side,

00:08:01.100 --> 00:08:04.015
not anymore in a sub-field.

00:08:04.015 --> 00:08:07.725
We'll make that view called BankWithdrawals.

00:08:07.725 --> 00:08:11.370
Now let's query all of that information.

00:08:11.370 --> 00:08:15.120
Let's see we've got some amounts to work with.

00:08:15.120 --> 00:08:18.710
What's our minimal transaction to be considered

00:08:18.710 --> 00:08:23.845
significant for this data, let's say $500.

00:08:23.845 --> 00:08:29.620
Let's do a select we will call it bankWithdrawsSelectDF,

00:08:32.510 --> 00:08:37.140
and we're going to use Spark.sql.

00:08:37.140 --> 00:08:43.605
Select all the fields from BankWithdrawals. What next?

00:08:43.605 --> 00:08:50.190
Where amount, what greater than $500?

00:08:50.190 --> 00:08:55.385
Because that's a flow, we can do that. There we go.

00:08:55.385 --> 00:08:58.760
Now we've got that data frame ready to be joined.

00:08:58.760 --> 00:09:03.730
We're limiting to $500 transactions or greater.

00:09:03.730 --> 00:09:08.460
Now we're going to get the next data frame from Kafka.

00:09:08.460 --> 00:09:15.340
So we're going to get the ATM messages, atmWithdrawalsRawStreamingDF.

00:09:20.840 --> 00:09:25.250
We're going to use the same Spark Session guys because we don't need

00:09:25.250 --> 00:09:29.150
more than one Spark Session ever in a single application.

00:09:29.150 --> 00:09:31.605
Getting to the readStream,

00:09:31.605 --> 00:09:34.555
making it format ("kafka"),

00:09:34.555 --> 00:09:38.135
sending the options to connect to the server.

00:09:38.135 --> 00:09:41.775
The topic is called atm-withdrawals.

00:09:41.775 --> 00:09:47.630
That information is coming from our simulated business application that we looked at

00:09:47.630 --> 00:09:54.255
earlier on to startingOffset to be earliest. Then we want to load it.

00:09:54.255 --> 00:09:56.800
We need to do a similar conversion here,

00:09:56.800 --> 00:10:02.825
so atmStreamingDF equals atmRawStreamingDF.Select

00:10:02.825 --> 00:10:11.770
Expr cast (key as string),

00:10:11.770 --> 00:10:14.640
and its transactionId again.

00:10:14.640 --> 00:10:17.735
Is it okay to have the same field name? Yes, it is.

00:10:17.735 --> 00:10:20.240
It's in a different data frame.

00:10:20.240 --> 00:10:23.030
So that'll be just fine.

00:10:23.030 --> 00:10:26.735
We're going to cast (value as a string) as well,

00:10:26.735 --> 00:10:34.295
and that's going to be called atmMessageJSON.

00:10:34.295 --> 00:10:38.695
Now this message data could be coming actually from a real ATM.

00:10:38.695 --> 00:10:39.955
In our case it's not,

00:10:39.955 --> 00:10:43.975
but ATMs generate a lot of interesting information.

00:10:43.975 --> 00:10:50.425
Now we're going to take this ATM streaming DataFrame and create

00:10:50.425 --> 00:10:58.015
an overridden column called ATM message JSON.

00:10:58.015 --> 00:11:04.885
We're going to extract data from the column that's already there by the same name.

00:11:04.885 --> 00:11:07.210
We're overloading a column.

00:11:07.210 --> 00:11:09.100
We need to use the right schema.

00:11:09.100 --> 00:11:12.640
The right schema here is ATM withdrawals message schema.

00:11:12.640 --> 00:11:18.085
Then we're going to select all the subfields that got written into that.

00:11:18.085 --> 00:11:22.000
The field again is ATM message JSON.

00:11:22.000 --> 00:11:25.330
We're going to do select column.

00:11:25.330 --> 00:11:31.645
That allows us to do this expression here, dot star.

00:11:31.645 --> 00:11:36.175
We're going create a view with that data in it called ATM withdrawals.

00:11:36.175 --> 00:11:40.495
Now we can go ahead and query that information.

00:11:40.495 --> 00:11:47.125
We're going to do another slight maneuver on this data.

00:11:47.125 --> 00:11:49.750
We're going to select that,

00:11:49.750 --> 00:11:56.560
but we want to alias the transaction ID.

00:11:56.560 --> 00:12:00.325
We don't have a collision on the join with the same field names.

00:12:00.325 --> 00:12:02.455
Because I said it was okay which it is.

00:12:02.455 --> 00:12:04.750
But until this point when we have to have them all live

00:12:04.750 --> 00:12:07.930
together than they have done their own names.

00:12:07.930 --> 00:12:10.990
I want the transaction date,

00:12:10.990 --> 00:12:16.780
I want the transaction ID as ATM transaction ID,

00:12:16.780 --> 00:12:20.335
I want ATM location.

00:12:20.335 --> 00:12:24.625
We're selecting it all from ATM withdrawals.

00:12:24.625 --> 00:12:27.235
That's the name of the view.

00:12:27.235 --> 00:12:30.265
Now we're going to create our join.

00:12:30.265 --> 00:12:37.570
We're going to call it ATM withdrawal DataFrame,

00:12:37.570 --> 00:12:44.785
and we're going do the left-hand of the joint B, the bank withdraws.

00:12:44.785 --> 00:12:48.865
Select DF, that's this one.

00:12:48.865 --> 00:12:51.160
Dot join.

00:12:51.160 --> 00:12:56.695
The thing that we're going to join on the right hand of the joint is ATM's like star DF.

00:12:56.695 --> 00:13:00.880
We need to tell it how to join it because it's not going to guess.

00:13:00.880 --> 00:13:04.000
We need to do our triple equals.

00:13:04.000 --> 00:13:10.330
We want the transaction id to equal ATM transaction ID.

00:13:10.330 --> 00:13:14.200
The first one comes from the bank withdrawals,

00:13:14.200 --> 00:13:24.530
second one comes from the schema because we've created an alias here to that field name.

00:13:26.610 --> 00:13:30.220
Now we're going to write

00:13:30.220 --> 00:13:37.630
this DataFrame that has the joint information into a new Kafka topic,

00:13:37.630 --> 00:13:41.620
and we're going to do that here.

00:13:41.620 --> 00:13:44.200
We normally, when we stream from Kafka,

00:13:44.200 --> 00:13:46.075
we start with a SparkSession.

00:13:46.075 --> 00:13:48.130
It's a little different because in this case

00:13:48.130 --> 00:13:50.800
we already have a DataFrame. We don't need one.

00:13:50.800 --> 00:13:58.570
We're starting with the DataFrame and then we take the DataFrame and stream it.

00:13:58.570 --> 00:14:02.185
We first have to get it back into JSON format.

00:14:02.185 --> 00:14:05.305
We've aggregate this data from two different views.

00:14:05.305 --> 00:14:08.140
We're not wasting our energy because now we've got it all

00:14:08.140 --> 00:14:12.010
together and we want to create a new JSON payload.

00:14:12.010 --> 00:14:15.640
Let's do that.

00:14:15.640 --> 00:14:19.585
We want to cast the transaction id,

00:14:19.585 --> 00:14:22.180
which will come from the first record from

00:14:22.180 --> 00:14:29.215
the bank withdrawals are casting that as a string and that will become our key.

00:14:29.215 --> 00:14:33.130
Then we're going put stuff to JSON.

00:14:33.130 --> 00:14:36.730
We want to select all the fields we're going to want,

00:14:36.730 --> 00:14:38.935
which is all of them.

00:14:38.935 --> 00:14:42.745
We're going to put that into the value field,

00:14:42.745 --> 00:14:45.460
and we're doing right stream.

00:14:45.460 --> 00:14:47.679
This created a DataFrame.

00:14:47.679 --> 00:14:53.050
Then we're changing that to do the right stream call and then we're formatting.

00:14:53.050 --> 00:14:58.435
As Kafka, we've got to provide the bootstrap server again.

00:14:58.435 --> 00:15:01.180
We want to create a new topic,

00:15:01.180 --> 00:15:04.585
let's call it withdrawals location.

00:15:04.585 --> 00:15:09.910
Because that's the interesting information we got from the ATM is the location.

00:15:09.910 --> 00:15:12.520
We've going to give it a checkpoint location,

00:15:12.520 --> 00:15:18.865
which is a writable directory that the Spark worker can write to.

00:15:18.865 --> 00:15:22.870
I've created, I've told it a file called Kafka check point.

00:15:22.870 --> 00:15:25.105
It'll create that dynamically.

00:15:25.105 --> 00:15:27.250
I don't have to create it.

00:15:27.250 --> 00:15:32.350
I'm calling dot start and then await termination.

00:15:32.350 --> 00:15:34.315
If all this runs properly,

00:15:34.315 --> 00:15:38.650
we will have data in a new topic called What?

00:15:38.650 --> 00:15:40.900
Withdrawals location.

00:15:40.900 --> 00:15:45.820
I'm going to go to my home workspace.

00:15:45.820 --> 00:15:50.860
I'm going to list and I have a script here called submit bank withdrawals.

00:15:50.860 --> 00:15:52.660
Because I'm in the folder home workspace,

00:15:52.660 --> 00:15:55.135
I can do dot slash, which is here.

00:15:55.135 --> 00:16:01.880
Then I can do Summit bank withdrawals dot SH.

00:16:02.310 --> 00:16:04.630
Let's see if it crashes.

00:16:04.630 --> 00:16:08.000
If it does, we'll just seem to fix it and run it again.

00:16:09.780 --> 00:16:15.760
Line 12, StructField, transaction ID, integer type.

00:16:15.760 --> 00:16:19.735
It looks like I forgot some parentheses there. Yes, I did.

00:16:19.735 --> 00:16:21.610
Let's see, can you look for me?

00:16:21.610 --> 00:16:23.755
Did I miss any others?

00:16:23.755 --> 00:16:26.335
Yeah, right, I didn't.

00:16:26.335 --> 00:16:29.780
We're going save that again.

00:16:30.870 --> 00:16:35.650
The Python file up arrow will rerun.

00:16:35.650 --> 00:16:42.745
We're going to run it one more time and see what we get. Line 48.

00:16:42.745 --> 00:16:47.470
Bank withdrawals message schema is not defined.

00:16:47.470 --> 00:16:57.970
Bank withdrawals message schema.

00:16:57.970 --> 00:17:01.270
The d is uppercase here.

00:17:01.270 --> 00:17:04.180
Let's make it not uppercase.

00:17:04.180 --> 00:17:06.830
Save that.

00:17:07.320 --> 00:17:16.840
Run it again. Line 48 calls should be column.

00:17:16.840 --> 00:17:22.100
Assertion error, COL should be column.

00:17:22.230 --> 00:17:26.870
What happened here is I'm missing an extra paren.

00:17:27.090 --> 00:17:35.570
I'm going to put that in. That should fix that. Up arrow again.

00:17:35.610 --> 00:17:44.900
Invalid syntax bank withdrawals message scheme.

00:17:45.120 --> 00:17:51.265
I'm selecting the bank withdrawal JSON field.

00:17:51.265 --> 00:17:54.970
I'm creating a new column inside of it.

00:17:54.970 --> 00:17:59.050
Oh, I missed something here.

00:17:59.050 --> 00:18:07.240
I need to say from JSON and then I need to pass in the field,

00:18:07.240 --> 00:18:14.840
I'm extracting it from bank withdrawal JSON,

00:18:16.830 --> 00:18:20.965
and then the name of the schema.

00:18:20.965 --> 00:18:27.080
Now I'm passing the JSON extracted into this field,

00:18:28.140 --> 00:18:37.375
save that ATM select star DF is not defined.

00:18:37.375 --> 00:18:40.450
See line 78.

00:18:40.450 --> 00:18:44.455
We've got our ATM streaming DataFrame with that column,

00:18:44.455 --> 00:18:47.600
we want to now create a temporary view.

00:18:48.150 --> 00:18:51.410
We did that here.

00:18:52.800 --> 00:18:59.305
We actually want to say equals Spark.SQL.

00:18:59.305 --> 00:19:08.950
Save that. Cannot be resolved on the left side of the join.

00:19:08.950 --> 00:19:10.450
So which side is the left site?

00:19:10.450 --> 00:19:13.075
This is at transaction ID.

00:19:13.075 --> 00:19:17.319
It saying that it doesn't recognize that field,

00:19:17.319 --> 00:19:20.830
which came from here,

00:19:20.830 --> 00:19:28.675
transaction ID plus side columns,

00:19:28.675 --> 00:19:32.620
account number, amount, date, and time.

00:19:32.620 --> 00:19:38.420
It does have a field called transaction ID.

00:19:38.850 --> 00:19:42.400
Let's see what we might have missed here.

00:19:42.400 --> 00:19:45.290
It could be Formatting.

00:19:45.660 --> 00:19:49.945
The issue that we just saw was caused by

00:19:49.945 --> 00:19:58.815
this expression not being properly surrounded by the EXPR function.

00:19:58.815 --> 00:20:03.155
We need to pass it in the EXPR function.

00:20:03.155 --> 00:20:06.460
That way it knows it's a select expression.

00:20:06.460 --> 00:20:08.110
That's what was failing.

00:20:08.110 --> 00:20:12.079
We're going to fix that.

00:20:12.180 --> 00:20:18.670
Then if I go to home workspace,

00:20:18.670 --> 00:20:28.980
and then I'm going to do dot slash submit bank withdrawals dot SH.

00:20:28.980 --> 00:20:31.994
Loading all the jars,

00:20:31.994 --> 00:20:34.955
waiting for the output.

00:20:34.955 --> 00:20:37.945
The output will go to Kafka.

00:20:37.945 --> 00:20:40.735
As long as we keep seeing that blinking cursor,

00:20:40.735 --> 00:20:45.815
we're still generating new messages in Kafka.

00:20:45.815 --> 00:20:48.030
I'm seeing that.

00:20:48.030 --> 00:20:53.630
This is a normal warning that we see with HDFS.

00:20:53.630 --> 00:20:56.350
We're going to keep on trucking.

00:20:56.350 --> 00:20:58.645
I'm going to go over here again.

00:20:58.645 --> 00:21:04.765
I'm going to go to the data Kafka bin folder listed.

00:21:04.765 --> 00:21:10.060
I'm going say dot slash Kafka console consumer,

00:21:10.060 --> 00:21:14.695
bootstrap server, local host 1992.

00:21:14.695 --> 00:21:23.135
The topic we want is that new topic we created withdrawals location,

00:21:23.135 --> 00:21:28.939
and from the beginning should be JSON formatted,

00:21:28.939 --> 00:21:31.640
and it should have the data from both.

00:21:31.640 --> 00:21:33.680
If I control C on that,

00:21:33.680 --> 00:21:35.250
we can look at one holding still.

00:21:35.250 --> 00:21:38.345
We have an account number and amount, date and time.

00:21:38.345 --> 00:21:41.900
That information came from the bank withdrawal.

00:21:41.900 --> 00:21:46.250
Then we have transaction ID as well.

00:21:46.250 --> 00:21:52.484
We have the ATM transaction ID and the ATM location in Thailand for this transaction.

00:21:52.484 --> 00:21:59.010
That was joining data and putting in JSON format into Kafka topic.

