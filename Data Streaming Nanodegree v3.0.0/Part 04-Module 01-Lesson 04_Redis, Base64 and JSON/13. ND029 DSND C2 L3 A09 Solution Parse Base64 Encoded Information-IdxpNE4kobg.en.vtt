WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.995
Let's look at the solution to the customer location exercise.

00:00:04.995 --> 00:00:08.880
First thing we need to do is start Kafka.

00:00:08.880 --> 00:00:11.595
If I say ps -ef,

00:00:11.595 --> 00:00:13.755
Kafka is not running.

00:00:13.755 --> 00:00:20.790
I'm going to start it up, and I'm also going to start Spark.

00:00:20.790 --> 00:00:29.265
Cd to home workspace spark sbin./start-master.sh.

00:00:29.265 --> 00:00:35.830
Copying the path to the log and I'm going to tail-f and the path to the log.

00:00:36.250 --> 00:00:42.790
Copy that again, tail-f and the path to the log,

00:00:42.790 --> 00:00:45.390
and there's the Spark Master.

00:00:45.390 --> 00:00:50.235
I'm going to break out of the tail, control C, ls,

00:00:50.235 --> 00:00:55.575
start-slave.sh and the master URI,

00:00:55.575 --> 00:00:58.320
and now Spark is running.

00:00:58.320 --> 00:01:02.655
Ps -ef, I see Spark and Spark.

00:01:02.655 --> 00:01:05.175
Kafka is up and running.

00:01:05.175 --> 00:01:09.915
The first thing I'm going to do is actually check the topic.

00:01:09.915 --> 00:01:13.850
I want to see if the data that should be listened to,

00:01:13.850 --> 00:01:18.180
the customer location, is showing up.

00:01:18.180 --> 00:01:22.494
I'm going to go to the cd data.

00:01:22.494 --> 00:01:25.435
I'm going to close this,

00:01:25.435 --> 00:01:29.960
and I'm going to clear the screen just to make it more readable.

00:01:29.960 --> 00:01:34.640
Cd data Kafka bin, ls,

00:01:34.640 --> 00:01:41.675
kafka-console-consumer bootstrap-server

00:01:41.675 --> 00:01:52.950
localhost:9092 topic redis-server from-beginning.

00:01:54.380 --> 00:02:04.250
What I'm looking for is a key for a collection for customer location.

00:02:04.250 --> 00:02:08.990
I'm going to actually clear this as well and I'm going to say,

00:02:08.990 --> 00:02:15.045
echo "CustomerLocation" pipe base64,

00:02:15.045 --> 00:02:18.540
and this is the key that I'm looking for.

00:02:18.540 --> 00:02:25.845
I'm going to scroll through and see if I can see that Q3.

00:02:25.845 --> 00:02:28.660
There we have one,

00:02:33.050 --> 00:02:35.745
Q3VzdGtcX, and it's the same.

00:02:35.745 --> 00:02:37.800
I just checked the first eight or 10 characters,

00:02:37.800 --> 00:02:38.970
which is a match.

00:02:38.970 --> 00:02:43.960
So we've got data and that's a good sign.

00:02:44.090 --> 00:02:49.490
I want to get access to that data and I'm going to convert it from base64 to

00:02:49.490 --> 00:02:58.820
non base64 and then I'm going to get the JSON data out of that decoded string.

00:02:58.820 --> 00:03:03.335
You'll notice I've already created the schema for the Redis message.

00:03:03.335 --> 00:03:07.255
Now, I'm going to create the customerLocationSchema,

00:03:07.255 --> 00:03:13.200
which needs to match the following fields: account number and location.

00:03:13.200 --> 00:03:16.935
I'm going to pass an array of StructFields,

00:03:16.935 --> 00:03:26.415
accountNumber, StringType, location, StringType as well.

00:03:26.415 --> 00:03:29.445
Now, I'm going to create a SparkSession

00:03:29.445 --> 00:03:33.170
and I'm going to name the application customer-location,

00:03:33.170 --> 00:03:36.355
getOrCreate, that'll start it.

00:03:36.355 --> 00:03:39.330
Saying my log level,

00:03:39.330 --> 00:03:42.450
and now I'm going to read from Kafka.

00:03:42.450 --> 00:03:44.970
I'm reading Redis data from Kafka.

00:03:44.970 --> 00:03:47.380
How is it that I'm doing that?

00:03:48.200 --> 00:03:55.600
The answer is it's because we have Kafka connect running with a Redis source connector.

00:03:55.600 --> 00:04:00.180
Creating a Redis server raw streaming

00:04:00.180 --> 00:04:08.055
DataFrame and I'm going to say equals my Spark session.

00:04:08.055 --> 00:04:13.500
Accessing the read string, format is Kafka,

00:04:13.500 --> 00:04:15.255
and I'm going to go

00:04:15.255 --> 00:04:20.745
option Kafka bootstrap servers

00:04:20.745 --> 00:04:26.760
and I'm going give it the one host that I need to connect to.

00:04:28.100 --> 00:04:32.000
Then I'm going to add the topic name,

00:04:32.000 --> 00:04:33.530
which is what again?

00:04:33.530 --> 00:04:37.465
All my Redis stuff is going to one topic, redis-server.

00:04:37.465 --> 00:04:42.110
I'm going to set the starting offsets to

00:04:42.110 --> 00:04:48.315
earliest and I'm going to load it into the data frame.

00:04:48.315 --> 00:04:57.030
Now, I've got Redis data and it is base64 encoded.

00:04:57.030 --> 00:05:05.660
I'm going to take that data and I'm going to parse it before I unencode it.

00:05:05.660 --> 00:05:13.450
My Redis server streaming data frame is going to take the Redis data,

00:05:13.450 --> 00:05:14.995
take the JSON out of it,

00:05:14.995 --> 00:05:17.815
and then we're going to unencode it.

00:05:17.815 --> 00:05:26.710
I'm using select expression to cast my binary information from Kafka as strings.

00:05:26.890 --> 00:05:28.970
I'm going to call the key,

00:05:28.970 --> 00:05:31.080
key, and the value, value.

00:05:31.080 --> 00:05:34.750
Now I'm taking that information that's no longer

00:05:34.750 --> 00:05:40.715
binary and I'm going to update the column called value,

00:05:40.715 --> 00:05:44.710
and I'm going to use from JSON to pull out

00:05:44.710 --> 00:05:51.864
the value and I'm going to use the Redis message schema,

00:05:51.864 --> 00:05:55.920
select, and I'm calling

00:05:55.920 --> 00:06:03.035
column and I'm pulling out all the value subfields that just got created from the JSON.

00:06:03.035 --> 00:06:09.985
I'm creating a temporary view and it's called RedisData.

00:06:09.985 --> 00:06:15.235
Now, I'm going to query that information,

00:06:15.235 --> 00:06:25.680
and this is my zSet entries encoded streaming data frame.

00:06:25.680 --> 00:06:30.225
I'm going to do a Spark SQL statement, select key,

00:06:30.225 --> 00:06:36.185
zSet entries0.element

00:06:36.185 --> 00:06:41.810
as customer location from Redis data.

00:06:41.820 --> 00:06:47.900
The next thing I'm going to do is unbase64 that information.

00:07:01.020 --> 00:07:07.425
ZSetDecodedEntriesStreamingDF equals zSetEntriesEncodedStreamingDF.withColumn.

00:07:07.425 --> 00:07:14.960
Customer location is the column that has the data I want to unbase64.

00:07:14.960 --> 00:07:17.850
I'm going to say unbase64

00:07:21.650 --> 00:07:28.460
zSetEntriesEncodedStreamingDF.customerLocation is the field

00:07:28.460 --> 00:07:31.925
that I'm pulling the data out of.

00:07:31.925 --> 00:07:36.395
I want to cast that as a string.

00:07:36.395 --> 00:07:44.180
The next thing I'm going to do is parse the JSON that's nested inside of that base64.

00:07:44.180 --> 00:07:48.755
We've just got access to it and we want to parse that.

00:07:48.755 --> 00:07:53.700
I'm going to say zSetDecodedEntriesStreamingDF.withColumn,

00:08:00.700 --> 00:08:09.075
customerLocation from JSON, customer location,

00:08:09.075 --> 00:08:14.010
and I'm going to parse the customerLocations schema,

00:08:14.010 --> 00:08:19.570
and I'm going to select using the column function,

00:08:19.570 --> 00:08:25.195
customerLocation.star, which are the JSON fields that just got generated,

00:08:25.195 --> 00:08:33.465
and then I'm going to create or replace temp view customerLocation.

00:08:33.465 --> 00:08:38.500
Now, I'm going to select all the data from that and call

00:08:38.500 --> 00:08:45.765
it customerLocationsStreamingDF equals spark.sql.

00:08:45.765 --> 00:08:50.425
Select star from customer location.

00:08:50.425 --> 00:09:00.455
Let's take a look at that,.writeStream.outputMode, append,

00:09:00.455 --> 00:09:07.860
format console.start.awaitTermination,

00:09:08.990 --> 00:09:13.035
and we're going to save that file.

00:09:13.035 --> 00:09:20.520
Let's run it. I'm going to break out of this topic, "Control C",

00:09:20.520 --> 00:09:23.870
and I'm going to clear this to get it easier to read

00:09:23.870 --> 00:09:26.900
and the CD to home workspace and ls there,

00:09:26.900 --> 00:09:33.360
and I have a script called submit-customer-location.sh.

00:09:35.800 --> 00:09:41.895
The null roles are data from Redis that doesn't match the schema,

00:09:41.895 --> 00:09:45.170
so when we applied the schema it didn't match

00:09:45.170 --> 00:09:49.970
the customer location so those rows do not match.

00:09:49.970 --> 00:09:56.405
One way to fix that is you can just say where location not null,

00:09:56.405 --> 00:09:58.810
like this, and you won't get those.

00:09:58.810 --> 00:10:02.000
Here we have an account that the customer is in Togo,

00:10:02.000 --> 00:10:03.335
we have one in France,

00:10:03.335 --> 00:10:05.270
one in Jordan, New Mexico, Nigeria,

00:10:05.270 --> 00:10:07.399
Ukraine and China, Mexico,

00:10:07.399 --> 00:10:09.040
France, and Ivory Coast.

00:10:09.040 --> 00:10:11.510
This is just keeping tabs on your customer where they

00:10:11.510 --> 00:10:14.525
are and this can be helpful information for

00:10:14.525 --> 00:10:17.270
determining fraud on a bank account or something like

00:10:17.270 --> 00:10:20.570
that if you see transactions that don't match.

00:10:20.570 --> 00:10:26.870
You've just taken data from a Redis Kafka source connector into Spark

00:10:26.870 --> 00:10:33.830
and then decoded the base64 in information and then extracted the JSON out of it.

00:10:33.830 --> 00:10:36.890
Good work. I'm going to kill this with control

00:10:36.890 --> 00:10:43.410
C and I'm going to kill this as well, and we're good.

