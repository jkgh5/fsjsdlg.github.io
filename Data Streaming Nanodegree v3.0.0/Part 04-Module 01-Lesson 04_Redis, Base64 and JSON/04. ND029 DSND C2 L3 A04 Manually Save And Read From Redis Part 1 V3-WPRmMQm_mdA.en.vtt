WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.390
Let's work on saving and reading from Redis.

00:00:03.390 --> 00:00:07.545
In this lesson we learn about manually saving and reading from Redis.

00:00:07.545 --> 00:00:10.605
We will first talk about Kafka as a central hub.

00:00:10.605 --> 00:00:13.980
Then we will discuss why streaming from Redis makes sense.

00:00:13.980 --> 00:00:17.530
Last, we will explore how to stream from Redis.

00:00:17.530 --> 00:00:21.300
Let's talk about Kafka as a central hub.

00:00:21.300 --> 00:00:24.420
We need to first remember that any data we read from

00:00:24.420 --> 00:00:27.480
a Kafka source originates from somewhere else.

00:00:27.480 --> 00:00:32.250
Data can come from databases, IoT devices,

00:00:32.250 --> 00:00:36.165
log files, financial applications,

00:00:36.165 --> 00:00:38.280
and many other sources.

00:00:38.280 --> 00:00:41.300
When reading information from a Kafka topic,

00:00:41.300 --> 00:00:44.015
the path that follows can look something like this.

00:00:44.015 --> 00:00:47.240
The source system transmit some data to Kafka.

00:00:47.240 --> 00:00:52.150
Kafka transmits that same data to a topic without modification.

00:00:52.150 --> 00:00:54.920
The sink or receiving system decodes

00:00:54.920 --> 00:00:58.850
the information that was encoded by the source system.

00:00:58.850 --> 00:01:04.880
Often the final consumer is responsible to transform the original system's data.

00:01:04.880 --> 00:01:09.485
The first message has a key called key and an encoded value.

00:01:09.485 --> 00:01:12.380
The decoded value is test key.

00:01:12.380 --> 00:01:17.285
The second message has a key called element and an encoded value.

00:01:17.285 --> 00:01:20.270
The decoded value is test value.

00:01:20.270 --> 00:01:22.700
Once the sink has decoded the data,

00:01:22.700 --> 00:01:25.300
it can be used for processing.

00:01:25.300 --> 00:01:30.860
Let's start by working with JSON Kafka messages as a Spark streaming source.

00:01:30.860 --> 00:01:33.305
We will first analyze the source sample.

00:01:33.305 --> 00:01:35.075
Next we create a schema.

00:01:35.075 --> 00:01:38.500
Last, we use the schema to extract fields.

00:01:38.500 --> 00:01:44.145
A core application is a system that is used to run critical processes for a business.

00:01:44.145 --> 00:01:49.080
Here is a sample diagram where the core application stores data in a database.

00:01:49.080 --> 00:01:52.110
When given this scenario we need to decide whether we

00:01:52.110 --> 00:01:54.630
want to modify the core application to

00:01:54.630 --> 00:02:01.320
transmit messages or instead use the database as the source of the messages.

00:02:01.320 --> 00:02:06.120
When faced with the decision in this example of whether to stream directly

00:02:06.120 --> 00:02:10.410
from the source as a new project or to stream from the current database,

00:02:10.410 --> 00:02:13.260
you should consider these implications.

00:02:13.260 --> 00:02:17.190
When streaming directly from the source the quality is high,

00:02:17.190 --> 00:02:21.180
this is because it is directly written by the core application.

00:02:21.180 --> 00:02:23.580
The time to value can be slower.

00:02:23.580 --> 00:02:28.245
Time to value is the time it takes for a customer to benefit from your service.

00:02:28.245 --> 00:02:31.890
The time to value can be slower due to coordination with

00:02:31.890 --> 00:02:35.985
the main development team who supports ongoing work on the core application.

00:02:35.985 --> 00:02:39.735
The team often has several high priority initiatives.

00:02:39.735 --> 00:02:43.845
The regression risk of modifying the core application is moderate,

00:02:43.845 --> 00:02:47.925
due to the critical role it has in performing business functions.

00:02:47.925 --> 00:02:51.660
Choosing an indirect source can make time to value lower,

00:02:51.660 --> 00:02:54.570
reduce development strain on the core team,

00:02:54.570 --> 00:02:57.450
and reduce risk to the main application.

00:02:57.450 --> 00:03:00.285
The indirect source may be lower quality data,

00:03:00.285 --> 00:03:03.660
because it is not directly produced by the core system.

00:03:03.660 --> 00:03:05.850
In this lesson we will be dealing with

00:03:05.850 --> 00:03:07.950
the sample banking application that

00:03:07.950 --> 00:03:11.235
currently uses Redis for storing customer information.

00:03:11.235 --> 00:03:14.340
Redis data is stored in various data types.

00:03:14.340 --> 00:03:18.555
A sorted set is a Redis collection that includes a value and a score.

00:03:18.555 --> 00:03:20.820
A score is a secondary index.

00:03:20.820 --> 00:03:24.480
We use ZADD to add records to a sorted set.

00:03:24.480 --> 00:03:28.785
A Redis key value pair is a simple key with one value.

00:03:28.785 --> 00:03:31.710
We use the set command to set the value.

00:03:31.710 --> 00:03:35.005
Let's start working with streaming from Redis.

00:03:35.005 --> 00:03:39.140
Let's discuss a phone number that is shared by multiple systems.

00:03:39.140 --> 00:03:43.385
The banking application writes the data as the system of record.

00:03:43.385 --> 00:03:45.800
The customer service system uses

00:03:45.800 --> 00:03:49.460
the information to help employees interact with customers.

00:03:49.460 --> 00:03:54.100
The auto dialer uses the phone number to make automated phone calls.

00:03:54.100 --> 00:03:56.900
In some scenarios it may be useful for one of

00:03:56.900 --> 00:04:01.340
the reading applications to be updated immediately when a phone number changes.

00:04:01.340 --> 00:04:05.960
For example, an auto dialer making phone calls may be in violation of

00:04:05.960 --> 00:04:11.465
the law if it repeatedly calls a phone number recently placed on a Do Not Call list.

00:04:11.465 --> 00:04:13.730
In this case, the secondary system,

00:04:13.730 --> 00:04:18.545
the auto dialer, should receive updates by subscribing to a Kafka topic.

00:04:18.545 --> 00:04:22.355
What if there is no Kafka topic currently transmitting the phone number?

00:04:22.355 --> 00:04:25.235
This is where we would leverage a Kafka source connector.

00:04:25.235 --> 00:04:27.740
We will configure Kafka Connect to connect to

00:04:27.740 --> 00:04:30.725
Redis using a Redis-specific source connector.

00:04:30.725 --> 00:04:35.165
The way we configure the Redis source connector is by updating the properties file.

00:04:35.165 --> 00:04:37.325
The file includes several properties.

00:04:37.325 --> 00:04:41.585
The topic is the topic where the data from Redis will be made available.

00:04:41.585 --> 00:04:45.305
In this case, the configured topic is called Redis server.

00:04:45.305 --> 00:04:52.345
The host, port, password and DB name are used to establish a connection with Redis.

00:04:52.345 --> 00:04:56.615
Kafka Connect is part of the Confluent Kafka distribution.

00:04:56.615 --> 00:05:02.390
It is the component responsible for providing a path from an external system, a source,

00:05:02.390 --> 00:05:04.115
to a Kafka topic,

00:05:04.115 --> 00:05:08.825
or from a topic to an external system, a sink.

00:05:08.825 --> 00:05:13.385
A source connector provides a connection from an outside system,

00:05:13.385 --> 00:05:16.385
a source, to a Kafka topic.

00:05:16.385 --> 00:05:18.950
Here's how a source connector works.

00:05:18.950 --> 00:05:21.335
Kafka starts the connector.

00:05:21.335 --> 00:05:24.905
The connector begins reading from the source.

00:05:24.905 --> 00:05:28.920
New records appear in the configured topic.

