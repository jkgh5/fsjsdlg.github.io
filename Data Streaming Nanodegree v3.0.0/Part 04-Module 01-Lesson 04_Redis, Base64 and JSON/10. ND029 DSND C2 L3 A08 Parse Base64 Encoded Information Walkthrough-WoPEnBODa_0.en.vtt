WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.790
Let's walk through an example of parsing base64 encoded data using Spark.

00:00:05.790 --> 00:00:09.765
The first thing I'm going to do is make sure that Spark is up and running.

00:00:09.765 --> 00:00:12.670
I'm going to type ps -ef.

00:00:12.670 --> 00:00:16.980
I see a bunch of Java processes which tells me Kafka is running.

00:00:16.980 --> 00:00:19.365
I also see Redis running,

00:00:19.365 --> 00:00:23.070
but I don't see any process that has the word Spark in it.

00:00:23.070 --> 00:00:26.475
So I'm going to start Spark in another terminal.

00:00:26.475 --> 00:00:34.980
I'm going to cd /home/workspace/spark/sbin,

00:00:34.980 --> 00:00:37.500
and I will see two commands that I need in here.

00:00:37.500 --> 00:00:41.610
I'm going to say./start-master.sh.

00:00:43.970 --> 00:00:47.970
I'm copying the log and I'm going to tail the log

00:00:47.970 --> 00:00:52.030
with tail -f and the log directory and filename.

00:00:52.030 --> 00:00:55.100
I see the spark URI.

00:00:55.100 --> 00:00:59.105
I'm copying that. I'm going to break out of tail,

00:00:59.105 --> 00:01:01.295
which won't hurt anything.

00:01:01.295 --> 00:01:07.830
I'm going to run the start-slave.sh and pass the master URI.

00:01:07.830 --> 00:01:09.795
Now I have Spark running,

00:01:09.795 --> 00:01:12.105
and if I run ps -ef,

00:01:12.105 --> 00:01:14.625
I see two Spark processes.

00:01:14.625 --> 00:01:17.240
That means we're ready to start coding.

00:01:17.240 --> 00:01:20.100
You'll notice something a little different in

00:01:20.100 --> 00:01:23.255
this starter code that we have for this exercise.

00:01:23.255 --> 00:01:26.420
I've already imported the libraries as usual,

00:01:26.420 --> 00:01:29.195
but I've also created a schema for you.

00:01:29.195 --> 00:01:32.450
This schema represents the format of messages

00:01:32.450 --> 00:01:36.280
that come from the Redis-Kafka source connector topic.

00:01:36.280 --> 00:01:40.700
The next thing I'm going to do is create a schema for

00:01:40.700 --> 00:01:46.650
the embedded reservation data that I expect to see coming through.

00:01:46.660 --> 00:01:51.000
The format of the message looks like this.

00:01:51.650 --> 00:01:54.620
I'm going to create a schema for that.

00:01:54.620 --> 00:02:01.220
It has a reservationId, customerName, truckNumber, reservationDate.

00:02:01.220 --> 00:02:05.065
I'm going to call this reservationSchema,

00:02:05.065 --> 00:02:08.440
and it's a StructType,

00:02:08.600 --> 00:02:13.675
and it accepts a array of struct fields.

00:02:13.675 --> 00:02:18.145
Each one needs to match the JSON field name.

00:02:18.145 --> 00:02:21.195
So we start with reservationId,

00:02:21.195 --> 00:02:23.985
and it's a StringType.

00:02:23.985 --> 00:02:30.120
The next one is going to be customerId and that's also a StringType.

00:02:30.120 --> 00:02:37.080
I'm actually going to paste in here a more complete JSON sample.

00:02:37.180 --> 00:02:40.509
I need to fix my spelling of customer.

00:02:40.509 --> 00:02:44.805
I actually have these fields: reservationId, customerId,

00:02:44.805 --> 00:02:50.040
customerName, truckNumber, reservationDate, checkInStatus, origin, and destination.

00:02:50.040 --> 00:02:55.425
The next field is called customerName, and it's a string.

00:02:55.425 --> 00:02:59.355
The field after that is truckNumber, string as well.

00:02:59.355 --> 00:03:02.035
The next one is a reservationDate,

00:03:02.035 --> 00:03:04.360
but we're going to treat it like a string.

00:03:04.360 --> 00:03:07.840
CheckinStatus is a string as well.

00:03:07.840 --> 00:03:12.365
The origin field is a string as well.

00:03:12.365 --> 00:03:15.740
The last field is a destination,

00:03:15.740 --> 00:03:17.695
which is a string as well.

00:03:17.695 --> 00:03:20.420
Now we're going to create our Spark session that we're going to

00:03:20.420 --> 00:03:23.410
use for the rest of this code.

00:03:23.410 --> 00:03:26.300
Remember, you're going to have to import this when you write

00:03:26.300 --> 00:03:29.765
your own Python because it won't just be there.

00:03:29.765 --> 00:03:33.340
We're going to call this truck-reservation.

00:03:33.340 --> 00:03:36.405
Let's set the logging level,

00:03:36.405 --> 00:03:39.300
and let's read from Kafka.

00:03:39.300 --> 00:03:49.290
I'm going to call this first data frame the reservationRawStreaming data frame.

00:03:49.290 --> 00:03:55.680
I'm going to call.readStream and.format,

00:03:55.680 --> 00:03:58.560
we'll call that Kafka format.

00:03:58.560 --> 00:04:08.310
We need to pass the option for kafka.bootstrap.servers with the host name and the port.

00:04:08.310 --> 00:04:11.700
The next option is the topic name,

00:04:11.700 --> 00:04:13.620
they call it subscribe,

00:04:13.620 --> 00:04:16.485
and it's called redis-server.

00:04:16.485 --> 00:04:19.485
We're going to say startingOffsets,

00:04:19.485 --> 00:04:24.105
starting at earliest. We're going to load it.

00:04:24.105 --> 00:04:27.690
Next, we're going to convert these from binary to string.

00:04:27.690 --> 00:04:34.750
So we'll create another data frame to hold that called reservationStreamingDF.

00:04:35.360 --> 00:04:43.245
We need to take the RawStreamingDF and do a select expression.

00:04:43.245 --> 00:04:46.890
We're going to cast the key as a string,

00:04:46.890 --> 00:04:51.070
and we'll call the key reservationId.

00:04:52.160 --> 00:04:56.850
Let's cast the value as a string as well.

00:04:56.850 --> 00:04:59.595
We'll call it reservationJson.

00:04:59.595 --> 00:05:04.775
Next, we want to take the JSON into independent fields.

00:05:04.775 --> 00:05:08.165
We need to start with the data frame we had last.

00:05:08.165 --> 00:05:10.910
We're going to call the withColumn method,

00:05:10.910 --> 00:05:15.765
and the name of the column we are working with is reservationJson.

00:05:15.765 --> 00:05:19.689
We're going to use a from_json method,

00:05:19.689 --> 00:05:25.000
and we're going to extract data from the reservationJson field.

00:05:25.000 --> 00:05:29.795
We're going to use the schema called reservationSchema.

00:05:29.795 --> 00:05:35.095
Then we're going to select the subfields.

00:05:35.095 --> 00:05:39.460
Then we're going to createOrReplaceTempView.

00:05:41.600 --> 00:05:48.345
The view we're going to call this is RedisData.

00:05:48.345 --> 00:05:51.240
Actually, I think we're going

00:05:51.240 --> 00:05:57.905
to change this from reservationId to key because we actually won't have that yet.

00:05:57.905 --> 00:06:01.919
We're just going to have a key and a value,

00:06:01.919 --> 00:06:05.805
but it's just the raw data.

00:06:05.805 --> 00:06:09.945
We're actually using the redisMessageSchema first.

00:06:09.945 --> 00:06:11.610
We're going to extract the JSON out of that,

00:06:11.610 --> 00:06:13.950
and then we're going to extract JSON again that's

00:06:13.950 --> 00:06:16.605
embedded inside which is the reservationSchema.

00:06:16.605 --> 00:06:20.980
We want to start with the redisMessageSchema actually.

00:06:22.220 --> 00:06:26.320
Now we're going to create a view called RedisData,

00:06:26.320 --> 00:06:28.990
and then we're going to select the data out of it.

00:06:28.990 --> 00:06:31.885
We want to get specifically the key,

00:06:31.885 --> 00:06:37.190
and then we want to get the first zSetEntry out of it.

00:06:37.190 --> 00:06:40.460
We're going to do that with a query.

00:06:40.820 --> 00:06:49.190
Let's change these to redisServerRawStreamingDF.

00:06:49.190 --> 00:06:56.555
This one is redisServerStreamingDF.

00:06:56.555 --> 00:07:00.345
This is redisServerRawStreamingDF.

00:07:00.345 --> 00:07:04.425
This is redisServerStreamingDF.

00:07:04.425 --> 00:07:07.170
Now we're going to do our query.

00:07:07.170 --> 00:07:17.805
We're going to call this zSetEntriesEncodedStreaming data frame

00:07:17.805 --> 00:07:20.885
because it has base64 data in it.

00:07:20.885 --> 00:07:23.720
We're going to use the spark.sql.

00:07:23.720 --> 00:07:25.655
We're going to select key,

00:07:25.655 --> 00:07:32.295
and zSetEntries 0 as

00:07:32.295 --> 00:07:37.840
the reservation from RedisData.

00:07:37.990 --> 00:07:47.310
Now we're going to go ahead and unbase64 the columns in here that we want.

00:07:47.310 --> 00:07:56.620
We're going to say zSetDecodedEntriesStreamingDF

00:07:57.470 --> 00:08:05.470
equals zSetEntriesEncodedStreamingDF.withColumn.

00:08:06.400 --> 00:08:10.730
We're modifying a column called reservation,

00:08:10.730 --> 00:08:13.010
which was base64 encoded,

00:08:13.010 --> 00:08:14.830
and soon it will not be.

00:08:14.830 --> 00:08:24.510
Unbase64 the zSetEntriesEncodedStreamingDF.

00:08:25.780 --> 00:08:28.950
and the field name.

00:08:29.030 --> 00:08:34.200
We're going to cast this as a string.

00:08:34.200 --> 00:08:40.320
Now we should have a decoded streaming data frame.

00:08:40.450 --> 00:08:44.945
The next thing we're going to do is create

00:08:44.945 --> 00:08:50.200
a view that has the JSON decoded out of base64.

00:08:50.200 --> 00:08:53.055
I mean, we're not in base64 anymore.

00:08:53.055 --> 00:08:58.215
So now we need to decode the non-encoded JSON.

00:08:58.215 --> 00:09:05.800
We're going to say zSetDecodedEntriesStreamingDF.withColumn.

00:09:07.550 --> 00:09:11.570
In here, we have a column called reservation,

00:09:11.570 --> 00:09:13.040
which is just JSON.

00:09:13.040 --> 00:09:16.985
Right now, we want to extract the JSON to separate fields.

00:09:16.985 --> 00:09:18.805
So we're going to say from_json,

00:09:18.805 --> 00:09:20.505
pass it the field name,

00:09:20.505 --> 00:09:25.740
and we do have a reservationSchema.

00:09:25.740 --> 00:09:27.270
We're going to use that.

00:09:27.270 --> 00:09:33.050
We're going to say.select and col.

00:09:33.050 --> 00:09:38.310
In here, we need to tell it the name of the fields we want;

00:09:38.310 --> 00:09:42.965
reservation.* all the fields.

00:09:42.965 --> 00:09:47.164
Then we're going to create a temporary view,

00:09:47.164 --> 00:09:51.890
and it's called TruckReservation.

00:09:51.890 --> 00:09:57.420
Now I'm going to say truckReservationStreaming.

00:09:58.710 --> 00:10:03.880
DF equals spark.sql.

00:10:03.880 --> 00:10:13.315
Select star from TruckReservation where reservationDate is not null.

00:10:13.315 --> 00:10:18.235
So we want all the non-null reservation dates,

00:10:18.235 --> 00:10:22.120
which basically means that the JSON parsed

00:10:22.120 --> 00:10:25.480
correctly and we got an actual reservation records,

00:10:25.480 --> 00:10:28.250
not something else from Redis.

00:10:29.070 --> 00:10:32.605
Now we're going to stream this to the console.

00:10:32.605 --> 00:10:45.010
We're going to say truckReservationStreamingDF.writeStream.outputMode,

00:10:45.010 --> 00:10:52.490
append.format is console, and then we're going to start it and await termination.

00:10:53.850 --> 00:11:00.835
I'm going to go to my home/workspace/spark/sbin folder,

00:11:00.835 --> 00:11:05.410
excuse me, home/workspace/spark folder

00:11:05.410 --> 00:11:11.180
list and I have a script submit-reservation-base64.sh.

00:11:16.470 --> 00:11:21.865
Line 51, invalid syntax.

00:11:21.865 --> 00:11:31.180
Spark. sparkContext.setLogLevel warn.

00:11:31.180 --> 00:11:36.055
Well one thing we can do is remove that line and see what happens after that.

00:11:36.055 --> 00:11:41.379
Save that. Invalid syntax,

00:11:41.379 --> 00:11:50.150
redisServerrawStreamingDF equals Spark.read stream.

00:11:50.160 --> 00:11:57.530
I have a feeling that my code is wrapping here in a weird way.

00:11:58.320 --> 00:12:02.755
I think this Spark didn't get instantiated.

00:12:02.755 --> 00:12:06.414
I'm going to get down one more line.

00:12:06.414 --> 00:12:12.175
The other thing I'm going to do is look at this file in the line editor,

00:12:12.175 --> 00:12:16.520
because I have a feeling that's part of the problem.

00:12:17.070 --> 00:12:21.730
I'm changing to the home workspace folder, I'm going to do and ls,

00:12:21.730 --> 00:12:29.885
and then I'm going to do vi reservation base64.py.

00:12:29.885 --> 00:12:33.010
Let's see what we have in here.

00:12:33.010 --> 00:12:38.335
I do have a Spark session instantiated.

00:12:38.335 --> 00:12:41.815
I have my SparkContext.

00:12:41.815 --> 00:12:48.170
I have my data frame equals spark.readStream.

00:12:48.630 --> 00:12:55.405
I see it. I didn't properly terminate the app name,

00:12:55.405 --> 00:12:59.480
so that would be a problem.

00:12:59.760 --> 00:13:04.945
Now I'm going to do colon q exclamation,

00:13:04.945 --> 00:13:11.170
and then I'm going to run the script again.

00:13:11.170 --> 00:13:20.395
Submit-reservation-base-64.sh. Then it says,

00:13:20.395 --> 00:13:25.360
unexpected character after line continuation character on line 62.

00:13:25.360 --> 00:13:30.440
I'm going to go back to my line editor.

00:13:39.480 --> 00:13:42.980
I'm going to quit out of here.

00:13:51.210 --> 00:13:54.500
Submit-reservation-base-64.sh.

00:13:57.360 --> 00:14:04.375
Unexpected character after line continuation character.

00:14:04.375 --> 00:14:15.020
Well, one thing we can do is get rid of the line continuation and to dot load directly.

00:14:21.510 --> 00:14:26.200
That happened because I modified it outside the editor.

00:14:28.610 --> 00:14:33.945
Line 70, invalid syntax.

00:14:33.945 --> 00:14:41.570
That should be a slash from JSON, line 70.

00:14:41.570 --> 00:14:44.455
We have from JSON,

00:14:44.455 --> 00:14:47.260
and then we have that.

00:14:47.260 --> 00:14:51.050
We were missing a print there.

00:14:56.310 --> 00:15:03.985
Line 79 on base-64, line 63.

00:15:03.985 --> 00:15:08.110
An error occurred when calling withColumn due to data type mismatch.

00:15:08.110 --> 00:15:11.725
Argument one requires string type.

00:15:11.725 --> 00:15:16.085
Reservation is a struct element type.

00:15:16.085 --> 00:15:19.390
Line 79.

00:15:20.420 --> 00:15:23.070
I'm just looking at this here,

00:15:23.070 --> 00:15:26.230
so we have streamingDF.reservation.

00:15:28.920 --> 00:15:37.045
Unbase-64, it's saying that this column has embedded data in it.

00:15:37.045 --> 00:15:39.160
Oh, I see this.

00:15:39.160 --> 00:15:41.875
ZSetEntries0.element.

00:15:41.875 --> 00:15:43.900
Because I wasn't parsing it,

00:15:43.900 --> 00:15:48.730
the element, I was parsing in the whole collection there.

00:15:48.730 --> 00:15:52.970
That's the reason I did that.

00:15:53.970 --> 00:15:57.415
Batch zero is showing no data.

00:15:57.415 --> 00:15:59.965
This is because I changed the log,

00:15:59.965 --> 00:16:02.515
sorry, the wrong one.

00:16:02.515 --> 00:16:08.800
Now one thing we need to probably check is if we have data

00:16:08.800 --> 00:16:14.230
going into that collection right now, as reservations.

00:16:14.230 --> 00:16:17.095
It looks like nothing's going in there.

00:16:17.095 --> 00:16:27.650
In the guide, we started Kafka using this button.

00:16:28.890 --> 00:16:39.775
I think if I go data/redis/redis-stable/src/ redis-cli -a not really.

00:16:39.775 --> 00:16:41.950
I do keys.

00:16:41.950 --> 00:16:45.670
If I say zrange, Reservation,

00:16:45.670 --> 00:16:49.765
zero minus one, we've got data,

00:16:49.765 --> 00:16:52.810
but we're not getting any new data.

00:16:52.810 --> 00:17:00.790
Sometimes what happens is the data that's getting read in from

00:17:00.790 --> 00:17:09.625
the Kafka source connector for Redis ends up being missed until Kafka Connect starts.

00:17:09.625 --> 00:17:14.215
If these records were written before Kafka Connect finished starting up,

00:17:14.215 --> 00:17:16.765
then they won't show up.

00:17:16.765 --> 00:17:24.745
One thing I can do is just take and zadd one of these.

00:17:24.745 --> 00:17:32.665
If I say zadd Reservation zero,

00:17:32.665 --> 00:17:38.690
and I'd changed the destination to Kentucky.

00:17:39.030 --> 00:17:42.370
That should create a record.

00:17:42.370 --> 00:17:49.340
There we go. Here's the data that we should've seen when things started up.

00:17:49.800 --> 00:17:52.420
We have a reservation ID,

00:17:52.420 --> 00:17:55.360
a customer ID, a customer name,

00:17:55.360 --> 00:17:58.465
a truck number, a reservation date,

00:17:58.465 --> 00:18:00.385
an empty check in status.

00:18:00.385 --> 00:18:02.995
If we look at the data we just parsed,

00:18:02.995 --> 00:18:07.700
it says check-in status as checked out.

00:18:09.090 --> 00:18:13.330
The reason that didn't properly parse, can you see it?

00:18:13.330 --> 00:18:19.075
It's the I, so I told you the StructField has to match exactly with the JSON.

00:18:19.075 --> 00:18:20.650
Because the I is off,

00:18:20.650 --> 00:18:22.825
it didn't catch that field.

00:18:22.825 --> 00:18:28.795
Then we have the origin of Wisconsin and the destination of Kentucky.

00:18:28.795 --> 00:18:33.220
That demonstrates reading data from a Kafka source connector that's

00:18:33.220 --> 00:18:37.300
connected to Redis in Spark on base-64,

00:18:37.300 --> 00:18:41.500
encoding the data and extracting the JSON as usable fields.

00:18:41.500 --> 00:18:45.250
I'm going to stop the Spark application here,

00:18:45.250 --> 00:18:49.100
and I'm going to exit out of the red cli.

