WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.970
现在你已了解如何将数据导入 dataframe 

00:00:02.970 --> 00:00:06.140
同时学会了基本的方法 现在我们开始处理一些数据吧

00:00:06.139 --> 00:00:10.800
我们会用到之前的音乐云服务日志数据集

00:00:10.800 --> 00:00:12.240
开始吧

00:00:12.240 --> 00:00:15.855
首先 从 pyspark 导入这些

00:00:15.855 --> 00:00:17.399
我已经导入好了

00:00:17.399 --> 00:00:22.484
稍后会用到这些函数和类型来进行计算

00:00:22.484 --> 00:00:26.879
我还导入了 datetime 以及 Numpy 和 pandas

00:00:26.879 --> 00:00:30.375
现在我们从 HDFS 中载入数据集

00:00:30.375 --> 00:00:33.645
用 sparky 读取 JSON 格式的方法读取 JSON 文件

00:00:33.645 --> 00:00:36.395
并将 dataframe 存入 user_log 这个变量中

00:00:36.395 --> 00:00:39.234
我们先看前五个记录

00:00:39.234 --> 00:00:40.755
正如我们之前看到的

00:00:40.755 --> 00:00:44.530
kenneth 正在听一首 “ Showaddywaddy ” 的歌

00:00:44.530 --> 00:00:46.295
记录有点难以阅读

00:00:46.295 --> 00:00:48.590
我们可以把框架打印出来

00:00:48.590 --> 00:00:51.875
这样我们可以看到dataframe 的所有列名了

00:00:51.875 --> 00:00:55.744
我们也可以对整个 dataframe 用 describe 方法来查看每列的计数

00:00:55.744 --> 00:00:59.570
平均值 标准差 最小值和最大值

00:00:59.570 --> 00:01:01.410
之前 我们使用这种方法时

00:01:01.409 --> 00:01:02.969
我们只看到了列名

00:01:02.969 --> 00:01:06.679
如果在后面加个 show 我们将看到实际的统计数据

00:01:06.680 --> 00:01:08.975
这有点难读

00:01:08.974 --> 00:01:12.339
所以我们可以按列挨个看

00:01:12.340 --> 00:01:15.090
例如 描述 artist 字段

00:01:15.090 --> 00:01:16.905
会得到这些统计数据

00:01:16.905 --> 00:01:19.905
暂时较难看出平均值和标准差

00:01:19.905 --> 00:01:22.144
在这个例子中的含义

00:01:22.144 --> 00:01:25.759
如果选择一个数值类型的列

00:01:25.760 --> 00:01:29.030
我们会看到更有意义的值

00:01:29.030 --> 00:01:30.680
可以看出

00:01:30.680 --> 00:01:33.845
我们有 10000 个 session ID

00:01:33.844 --> 00:01:36.629
值在 9 - 7144 之间

00:01:38.349 --> 00:01:42.304
这里也有平均值和标准差

00:01:42.305 --> 00:01:44.750
要查看 dataframe 中共有多少行

00:01:44.750 --> 00:01:47.689
可以用 count 方法

00:01:47.689 --> 00:01:50.284
这里一共有 10000 行

00:01:50.284 --> 00:01:53.009
所以 所有的记录都有 session ID

00:01:53.010 --> 00:01:55.245
而并不是所有的记录

00:01:55.245 --> 00:01:58.260
都有 artist 字段

00:01:58.260 --> 00:02:01.190
这是有道理的 

00:02:01.189 --> 00:02:03.109
因为如果记录首先应该指的是播放歌曲的记录

00:02:03.109 --> 00:02:05.254
而不是应该是 artist 记录

00:02:05.254 --> 00:02:08.674
接下来让我们看看请求的页面有哪些

00:02:08.675 --> 00:02:11.075
通过查看page字段

00:02:11.074 --> 00:02:14.209
使用 drop duplicates 

00:02:14.210 --> 00:02:16.760
这样每个页面只会出现一次了

00:02:16.759 --> 00:02:21.125
并根据页面字段进行排序

00:02:21.125 --> 00:02:26.224
按字母顺序进行排序

00:02:26.224 --> 00:02:29.299
用户可以登录和退出

00:02:29.300 --> 00:02:31.250
他们可能会访问主页

00:02:31.250 --> 00:02:34.699
这里是主页 或播放一首歌

00:02:34.699 --> 00:02:37.409
稍后我们会聚焦那些

00:02:37.409 --> 00:02:39.750
将付费帐户降级为免费帐户的用户

00:02:39.750 --> 00:02:43.870
那么 submit downgrades 字段里会有一些评论

00:02:43.870 --> 00:02:47.539
我们可以通过筛选特定用户 ID

00:02:47.539 --> 00:02:51.034
 获取其相关事件

00:02:51.034 --> 00:02:56.039
让我们选择几个列名以及用户 ID 然后并收集其所有事件

00:02:56.389 --> 00:03:02.018
正如我们在这里看到的 用户 Kenneth 听了几首歌

00:03:02.019 --> 00:03:06.740
访问了主页 然后在这里又听了几首歌

00:03:06.740 --> 00:03:10.990
这就是一个典型的用户事件日志的样子

00:03:10.990 --> 00:03:13.129
现在我们对

00:03:13.129 --> 00:03:16.564
用户在特定小时内听歌数量很感兴趣

00:03:16.564 --> 00:03:19.250
要计算这个 首先我们需要把

00:03:19.250 --> 00:03:21.909
时间戳从 epoch time 格式 转换为 datetime 格式

00:03:21.909 --> 00:03:24.484
这样我们才能获取时间里的具体小时数字

00:03:24.485 --> 00:03:31.355
为此 我们将定义一个名为 get hour 的自定义函数

00:03:31.354 --> 00:03:37.750
使用get hour 我们可以在 dataframe 添加一个名为 hour 的新列

00:03:37.750 --> 00:03:40.794
因为 Spark 采用惰性评估

00:03:40.794 --> 00:03:42.694
目前为止它还没做任何事

00:03:42.694 --> 00:03:45.769
但是如果我们使用 head 方法

00:03:45.770 --> 00:03:48.710
小时值就会被计算出来

00:03:48.710 --> 00:03:51.745
至少第一个记录的值会计算出来

00:03:51.745 --> 00:03:57.170
这首歌是晚上10点播放的

00:03:57.169 --> 00:04:01.054
下一步是计算一天中不同小时的

00:04:01.055 --> 00:04:04.050
歌曲页面请求数

00:04:04.219 --> 00:04:07.379
我们把结果排个序

00:04:07.379 --> 00:04:10.280
这样一天的时间就按顺序排列了

00:04:10.280 --> 00:04:15.395
接下来我们还需要加上 show 方法来查看统计结果

00:04:15.395 --> 00:04:17.660
接下来我们要把结果可视化

00:04:17.660 --> 00:04:20.610
我们可以把这个小 Spark dataframe 转换为

00:04:20.610 --> 00:04:25.520
Pandas dataframe 并使用 matplotlib 绘制图形

00:04:25.519 --> 00:04:30.289
我们还可以让图表更漂亮一点

00:04:30.290 --> 00:04:34.260
从图中 我们可以看到

00:04:34.259 --> 00:04:39.394
用户在一天中的不同时段播放歌曲的趋势

00:04:39.394 --> 00:04:42.919
在继续汇总 dataframe 里的其他数据前

00:04:42.920 --> 00:04:45.875
我们要先确保 dataframe 中没有任何缺失值

00:04:45.875 --> 00:04:49.370
正如我们之前看到的 一些字段里就应该是空值

00:04:49.370 --> 00:04:52.699
但我们要确保

00:04:52.699 --> 00:04:56.904
例如 所有记录的用户ID和 sessionid 都不为空

00:04:56.904 --> 00:04:59.079
使用 drop any 方法

00:04:59.079 --> 00:05:03.439
我们可以删除这些字段中为空的记录

00:05:03.439 --> 00:05:05.300
如果再次对行计数

00:05:05.300 --> 00:05:07.730
我们会发现我们没有删除任何一行

00:05:07.730 --> 00:05:10.825
那么让我们来看看用户ID

00:05:10.824 --> 00:05:15.349
用户ID列中没有空值

00:05:15.350 --> 00:05:20.495
这里有一个可疑的值 一个空字符串

00:05:20.495 --> 00:05:22.699
我们把用户ID为

00:05:22.699 --> 00:05:25.805
空字符串的行都过滤掉

00:05:25.805 --> 00:05:28.105
如果我们再次计算行数

00:05:28.105 --> 00:05:32.045
会看到我们删除了大约340条记录

00:05:32.045 --> 00:05:33.949
这样 我们就确保了

00:05:33.949 --> 00:05:37.779
所有用户 ID 值都是有效的

00:05:37.779 --> 00:05:40.504
在我们的计算中 我们可能需要区分

00:05:40.504 --> 00:05:43.774
用户在特定事件前后的行为数据

00:05:43.774 --> 00:05:45.919
例如 区分用户

00:05:45.920 --> 00:05:49.555
把帐户从付费帐户降级为免费帐户前后的活动

00:05:49.555 --> 00:05:53.180
让我们先把降级服务的用户找出来

00:05:53.180 --> 00:05:57.605
显然 用户 ID 1138

00:05:57.605 --> 00:06:02.254
一位名叫凯利的用户降级了她的服务

00:06:02.254 --> 00:06:06.389
那我们把她的用户活动列出来

00:06:06.389 --> 00:06:10.719
同时我们把她的用户级别字段也包括进来

00:06:10.720 --> 00:06:14.810
可以看到一开始她

00:06:14.810 --> 00:06:18.084
在使用付费服务来听歌的

00:06:18.084 --> 00:06:21.589
过了一段时间 她降级了服务

00:06:21.589 --> 00:06:25.699
成了一名免费用户

00:06:25.699 --> 00:06:28.579
我们最终目标是新建一个字段

00:06:28.579 --> 00:06:30.919
它会基于特殊的事件

00:06:30.920 --> 00:06:33.629
使用不同值来区分是事件前还是事件后

00:06:33.629 --> 00:06:35.985
我们称这个字段为 phase

00:06:35.985 --> 00:06:39.920
用户 phase 字段里

00:06:39.920 --> 00:06:44.455
在降级服务前后的值会不一样

00:06:44.454 --> 00:06:46.879
要创建 phase 首先我们把

00:06:46.879 --> 00:06:49.439
关于特殊事件的记录标记出来

00:06:49.439 --> 00:06:52.985
在我们的案例中 这个特殊事件就是降级服务

00:06:52.985 --> 00:06:57.035
我们先创建一个名为 downgrade 的字段

00:06:57.035 --> 00:07:00.510
如果该事件指的是 降级服务

00:07:00.509 --> 00:07:06.024
那字段里的值就是 1 否则为 0 

00:07:06.024 --> 00:07:11.209
好啦 我刚刚定义了这个函数 flag_downgrade_event

00:07:11.209 --> 00:07:14.659
函数会根据事件是否是 提交降级

00:07:14.660 --> 00:07:18.290
返回整数 1 或 0

00:07:18.290 --> 00:07:23.290
如果我们看一下 user_log_valid 的第一行

00:07:23.290 --> 00:07:26.080
我们可以看到 downgrade 字段出现了

00:07:26.079 --> 00:07:29.754
这行中 它的值为 0 

00:07:29.754 --> 00:07:33.024
要计算 phase 我们可以使用一个小技巧

00:07:33.024 --> 00:07:37.419
如果按逆时间顺序对特定用户的记录进行排序

00:07:37.420 --> 00:07:42.925
并对降级列中的值依此求和 直到发生特定事件那行

00:07:42.925 --> 00:07:44.829
我们可以用它作为 phase

00:07:44.829 --> 00:07:47.634
这完全满足我们的要求

00:07:47.634 --> 00:07:53.214
你可以在这节视频后找到更多原理解释

00:07:53.214 --> 00:07:57.804
我们先把行按逆时间顺序排列

00:07:57.805 --> 00:08:05.030
然后使用窗口函数把特定行之前的数据加总

00:08:05.029 --> 00:08:08.344
我忘了导入窗口函数了

00:08:08.345 --> 00:08:11.255
那我们先导入函数

00:08:11.254 --> 00:08:16.490
现在我定义这么一个窗口函数 然后我们根据

00:08:16.490 --> 00:08:19.160
用户ID 来分组

00:08:19.160 --> 00:08:25.050
按逆时间顺序排序

00:08:25.050 --> 00:08:28.650
加总降级行之上的所有行

00:08:28.649 --> 00:08:32.054
降级行之下的行都不加总

00:08:32.054 --> 00:08:35.009
现在我们就可以用这个窗口函数

00:08:35.009 --> 00:08:38.024
来计算 phase 字段了

00:08:38.024 --> 00:08:41.529
让我们再次选择 kelly 的活动

00:08:41.529 --> 00:08:45.889
我添加了时间戳和 phase 字段

00:08:45.889 --> 00:08:50.884
让我们按时间戳对这个 dataframe 进行排序

00:08:50.884 --> 00:08:54.125
现在如果我们看一下记录

00:08:54.125 --> 00:08:56.990
降级事件之上的所有事件

00:08:56.990 --> 00:09:00.595
的 phase 字段的值都变成了 1

00:09:00.595 --> 00:09:02.769
如果我们向下滚动

00:09:02.769 --> 00:09:06.875
提交降级这个事件下面的行

00:09:06.875 --> 00:09:09.470
的 phase 字段都变成了 0 

00:09:09.470 --> 00:09:11.345
如果我们有多个降级事件

00:09:11.345 --> 00:09:16.170
我们将看到 phase 值从n减小到零

