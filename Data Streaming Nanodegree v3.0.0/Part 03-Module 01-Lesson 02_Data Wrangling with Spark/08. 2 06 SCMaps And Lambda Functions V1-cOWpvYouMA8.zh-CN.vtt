WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.349
先来看一下 Spark 最常见的函数 Maps

00:00:04.349 --> 00:00:07.529
Maps 会复制原始输入数据

00:00:07.530 --> 00:00:09.330
并把副本数据

00:00:09.330 --> 00:00:11.550
按照你在 Map 中的函数进行转换

00:00:11.550 --> 00:00:15.150
Map 这个术语来自数学里的概念

00:00:15.150 --> 00:00:18.570
将输入映射到输出 而不是指地图

00:00:20.684 --> 00:00:23.160
不过你也可以把它看作是

00:00:23.160 --> 00:00:26.789
把数据从输入数据变换到输出数据的指引

00:00:26.789 --> 00:00:29.699
在 notebook 中输入一些 Spark 的初始化设置后

00:00:29.699 --> 00:00:33.710
我们把歌曲日志 —— 一个普通的 Python 列表

00:00:33.710 --> 00:00:36.560
转换为 Spark 可以使用的分布式数据集

00:00:36.560 --> 00:00:40.015
这歌转换用到了 SparkContext 这个对象

00:00:40.015 --> 00:00:42.795
通常缩写为SC

00:00:42.795 --> 00:00:46.190
SparkContext 有一个叫 parallelize 的方法

00:00:46.189 --> 00:00:48.739
这个方法会把一个 Python 对象分布到

00:00:48.740 --> 00:00:50.399
群集中的计算机中

00:00:50.399 --> 00:00:53.479
这样 Spark 就可以把它的函数用在数据集上了

00:00:53.479 --> 00:00:57.109
一旦我们可以把 Spark 用在这个小数据集时

00:00:57.109 --> 00:00:58.789
我们就可以开始处理它了

00:00:58.789 --> 00:01:01.879
一个简单的例子就是把大写的歌曲标题转换为小写

00:01:01.880 --> 00:01:04.579
数据集通常都会经过这样的预处理步骤

00:01:04.579 --> 00:01:06.200
使数据标准化

00:01:06.200 --> 00:01:08.465
标准化数据有助于歌曲次数的正确统计

00:01:08.465 --> 00:01:13.070
因为有些人用大写字母输入歌曲名称

00:01:13.069 --> 00:01:16.024
而有些人使用小写输入

00:01:16.025 --> 00:01:19.280
为此 我们可以写一个很简单的 Python 函数

00:01:19.280 --> 00:01:21.230
将歌曲名称都转换为小写

00:01:21.230 --> 00:01:24.895
我们用到的函数是 Python 内置的 lower 函数

00:01:24.894 --> 00:01:27.530
接下来 我们将使用 Spark 函数 map

00:01:27.530 --> 00:01:30.590
把 converst_song_to_lowercase 这个函数应用于

00:01:30.590 --> 00:01:32.719
数据集中的每首歌曲中

00:01:32.719 --> 00:01:34.609
所有这些步骤看上去是立即运行的

00:01:34.609 --> 00:01:37.129
但实际上

00:01:37.129 --> 00:01:40.004
spark 使用的是延迟计算

00:01:40.004 --> 00:01:42.890
Spark 还没有将歌曲转换为小写

00:01:42.890 --> 00:01:48.034
到目前为止 Spark 还在等待将歌曲转换为小写

00:01:48.034 --> 00:01:52.060
因为可能会有其他处理工作 如删除标点符号

00:01:52.060 --> 00:01:56.150
Spark希望等到最后 看看是否可以简化工作

00:01:56.150 --> 00:02:00.035
若可以 Spark 会将这些工作先合成一个阶段，再去处理数据

00:02:00.034 --> 00:02:03.069
如果我们想强制让 Spark 对数据进行操作

00:02:03.069 --> 00:02:06.199
我们可以使用 Collect 函数

00:02:06.200 --> 00:02:07.909
这个函数会把集群中的所有机器的结果

00:02:07.909 --> 00:02:10.115
收集到运行 notebook 的机器上

00:02:10.115 --> 00:02:15.100
另外 Spark 并没有改变原始数据集

00:02:15.099 --> 00:02:17.389
Spark 会拷贝原始数据集

00:02:17.389 --> 00:02:21.904
保持原始分布歌曲日志的大写字母不变

00:02:21.905 --> 00:02:24.169
这就是 Maps  的过程 很简单

00:02:24.169 --> 00:02:27.039
但在 Spark 编程中有重要作用

00:02:27.039 --> 00:02:29.569
如果你和我一样觉得

00:02:29.569 --> 00:02:32.209
创建一个全新的 Python 函数

00:02:32.210 --> 00:02:34.040
来转换歌曲名称的大小写太过麻烦

00:02:34.039 --> 00:02:36.859
你也可以直接用 Python 内置的 lower 函数

00:02:36.860 --> 00:02:38.990
因为用于定义和调用函数的代码

00:02:38.990 --> 00:02:41.210
比它里面的核心逻辑还要长

00:02:41.210 --> 00:02:43.460
幸运的是 Python有一个很赞的函数

00:02:43.460 --> 00:02:46.340
该函数来自函数式编程 叫做匿名函数

00:02:46.340 --> 00:02:50.300
它不需要给小块代码定义新函数名称

00:02:50.300 --> 00:02:52.850
在Python中使用匿名函数时

00:02:52.849 --> 00:02:55.549
用 Lambda 这个特殊的关键字

00:02:55.550 --> 00:02:58.969
然后写函数的输入 

00:02:58.969 --> 00:03:00.634
后跟冒号和期望的输出

00:03:00.634 --> 00:03:03.139
换句话说 冒号的左边是

00:03:03.139 --> 00:03:06.244
函数的输入参数

00:03:06.245 --> 00:03:09.439
右边是函数的返回结果

00:03:09.439 --> 00:03:11.569
因为这个方法很方便

00:03:11.569 --> 00:03:14.659
所以匿名函数在 Spark 里用的非常频繁

00:03:14.659 --> 00:03:18.604
至于你是否要用匿名函数 完全取决于你的偏好

00:03:18.604 --> 00:03:22.000
但有很多最佳案例和小例子都会使用匿名函数

00:03:22.000 --> 00:03:28.145
此外  很多示例会用更简洁的输入名称 如X 

00:03:28.145 --> 00:03:31.189
但是 我个人偏好输入名称内容更有意义些

00:03:31.189 --> 00:03:33.859
因为这种名称会更利于阅读代码和理解

00:03:33.860 --> 00:03:37.410
整个函数对数据处理的逻辑

