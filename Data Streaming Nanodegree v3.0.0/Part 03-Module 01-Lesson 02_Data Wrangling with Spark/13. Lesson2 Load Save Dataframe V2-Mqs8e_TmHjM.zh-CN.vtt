WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.044
在这个视频中 我们将通过一个例子

00:00:03.044 --> 00:00:06.794
来介绍如何从 Spark 导入和导出数据表

00:00:06.794 --> 00:00:08.685
让我们开始吧

00:00:08.685 --> 00:00:11.804
先看一下 SparkSession

00:00:11.804 --> 00:00:14.129
就像之前的视频中讨论的那样

00:00:14.130 --> 00:00:15.705
我们可以给它设置参数

00:00:15.705 --> 00:00:18.839
例如 我们可以给应用程序添加名称

00:00:18.839 --> 00:00:22.440
我们先导入 SparkSession

00:00:22.440 --> 00:00:24.225
getOrCreate 会更新

00:00:24.225 --> 00:00:26.970
已有的 SparkSession

00:00:26.969 --> 00:00:28.515
如果 SparkSession 不存在

00:00:28.515 --> 00:00:32.174
新的 SparkSession 就会被创建

00:00:32.174 --> 00:00:35.369
我们看看参数有啥变化

00:00:35.369 --> 00:00:39.839
我刚打错字了 这应该是大写字母C

00:00:39.840 --> 00:00:42.590
这里可以看到所有参数

00:00:42.590 --> 00:00:45.950
我们更新了应用程序的名称

00:00:45.950 --> 00:00:50.285
这个新的名称叫 our first Spark SQL example

00:00:50.284 --> 00:00:53.750
这里还可以查看其他参数

00:00:53.750 --> 00:00:57.515
我们现在把 JSON 文件加载到 Spark dataframe

00:00:57.515 --> 00:01:00.770
在接下来的几个视频中 我们会使用

00:01:00.770 --> 00:01:04.540
一个音乐云服务的日志数据集

00:01:04.540 --> 00:01:08.035
当用户使用应用程序的不同页面时

00:01:08.034 --> 00:01:10.414
他们对页面的请求会被记录下来

00:01:10.415 --> 00:01:16.455
我们先定义一个名叫 sparkify_log_small.json 的路径

00:01:16.454 --> 00:01:22.474
我之前已经将这个文件保存到运行 Spark 集群上的HDFS里了

00:01:22.474 --> 00:01:25.549
这是主节点的IP地址

00:01:25.549 --> 00:01:29.914
我们先将 JSON 文件加载到名为 user_log 的 dataframe 中

00:01:29.915 --> 00:01:31.550
要看结果的话

00:01:31.549 --> 00:01:35.015
我们可以用 printSchema 方法把提纲（schema）打印出来

00:01:35.015 --> 00:01:38.435
这里有几个描述用户的字段

00:01:38.435 --> 00:01:41.465
例如 用户ID

00:01:41.465 --> 00:01:43.655
名字和姓氏

00:01:43.655 --> 00:01:47.284
并且还有关于请求的信息 例如

00:01:47.284 --> 00:01:49.879
用户访问的页面信息

00:01:49.879 --> 00:01:54.199
HTTP方法或请求的状态

00:01:54.200 --> 00:01:58.520
让我们尝试一下 describe 方法 看看能得到啥信息

00:01:58.519 --> 00:02:03.289
如你所见 describe 返回了这个 dataframe 的列名以及字段类型

00:02:03.290 --> 00:02:07.100
和 printSchema 差不多

00:02:07.099 --> 00:02:10.055
我们还可以查看特定行的信息

00:02:10.055 --> 00:02:13.474
那我们看看第一行长啥样

00:02:13.474 --> 00:02:15.829
显然 用户 Kenneth 在听一首叫 showaddywaddy 的歌

00:02:15.830 --> 00:02:19.355
Was just listening to a, showaddywaddy.

00:02:19.354 --> 00:02:23.719
我们也可以使用 take 方法来获取前几条记录

00:02:23.719 --> 00:02:25.609
第一条记录和之前相同

00:02:25.610 --> 00:02:29.525
但我们可以多看几条数据 从而对数据集有更清晰的把握

00:02:29.525 --> 00:02:32.780
现在我们已成功将数据加载到 dataframe 中

00:02:32.780 --> 00:02:35.759
接下来我们看看如何将其保存为不同的格式

00:02:35.759 --> 00:02:38.280
例如 保存为 CSV 文件

00:02:38.280 --> 00:02:42.169
我把输出文件保存在和输入文件同一个 HDFS 文件夹里

00:02:42.169 --> 00:02:44.669
在这里命名输出文件名

00:02:44.669 --> 00:02:50.750
我把这个 CSV 文件命名为 sparkify_log_file.csv

00:02:50.750 --> 00:02:55.319
我们可以使用 write.save 方法来保存这个文件

00:02:55.319 --> 00:02:57.974
将格式设为 CSV

00:02:57.974 --> 00:03:00.549
并将标题设置为 true

00:03:00.550 --> 00:03:04.460
确保我们保留了列名称

00:03:04.460 --> 00:03:07.099
好了 现在我们加载一下

00:03:07.099 --> 00:03:09.939
刚刚保存的 CSV 文件

00:03:09.939 --> 00:03:14.759
我们可以使用 read.csv 方法 

00:03:14.759 --> 00:03:17.639
设置文件路径为我们之前保存CSV 文件的路径

00:03:17.639 --> 00:03:22.500
并将 header 设置为 true

00:03:22.500 --> 00:03:25.240
我们先来看看我们的框架

00:03:25.240 --> 00:03:27.545
和以前完全相同

00:03:27.544 --> 00:03:31.864
我们再看一下前几条记录

00:03:31.865 --> 00:03:35.060
同样 与我们之前看到的 dataframe 相同

00:03:35.060 --> 00:03:38.199
和预期一致

00:03:38.199 --> 00:03:41.179
在这个视频中 我们通过了一个例子

00:03:41.180 --> 00:03:45.125
学习了如何从 HDFS 读取和存储 dataframe

00:03:45.125 --> 00:03:47.145
如果文件存储在 S3 中

00:03:47.145 --> 00:03:48.795
你也可以使用相同的方法

00:03:48.794 --> 00:03:50.534
关于文件路径

00:03:50.534 --> 00:03:52.969
我们只需要确保

00:03:52.969 --> 00:03:56.939
目标文件的路径是正确的

